{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madaime2/Novel_Pollen_Phylogenetic_Placement/blob/main/Morphological_Visualizations/01_Morphological_Visualizations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23e5f368",
      "metadata": {
        "id": "23e5f368"
      },
      "outputs": [],
      "source": [
        "# BEGIN HERE (NAIVE CLASSIFIER)\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision \n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "mean = np.array([0.5, 0.5, 0.5])\n",
        "std = np.array([0.25, 0.25, 0.25])\n",
        " \n",
        "torch.manual_seed(3)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "       # transforms.RandomResizedCrop(224),\n",
        "         torchvision.transforms.Resize((224,224)),\n",
        " \n",
        "        torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
        "        torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
        "        torchvision.transforms.RandomRotation((-90,90)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "       # transforms.Resize(256),\n",
        "       torchvision.transforms.Resize((224,224)),\n",
        "       # transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = 'C:/Users/meada/OneDrive/Desktop/Podocarpus_Stacks_Split4/'\n",
        "\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=10,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b7b9af",
      "metadata": {
        "id": "02b7b9af"
      },
      "source": [
        "# Extract individual cross-sectional features while keeping MIP and Patch Features Fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ce85706",
      "metadata": {
        "id": "1ce85706",
        "outputId": "69572475-a1c8-46c3-a74f-33036b0ce83d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(20, 2048)\n",
            "(8, 2048)\n",
            "(11, 2048)\n",
            "(6, 2048)\n",
            "(11, 2048)\n",
            "(8, 2048)\n",
            "(17, 2048)\n",
            "(11, 2048)\n",
            "(3, 2048)\n"
          ]
        }
      ],
      "source": [
        "# Load na誰ve classifier (Cross-sectional Images) \n",
        "# ResNeXt101, pretrained on imagenet-1K dataset\n",
        "\n",
        "\n",
        "# Load na誰ve classifier (Cross-sectional Images) \n",
        "# ResNeXt101, pretrained on imagenet-1K dataset\n",
        "\n",
        "\n",
        "\n",
        "PATH = \"C:/Users/meada/OneDrive/Desktop/Podocarpus_Stacks_Split4.pt\"\n",
        "\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 30) \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);\n",
        "\n",
        "\n",
        "\n",
        "from torchvision.transforms.transforms import ConvertImageDtype\n",
        "from torchvision.transforms.functional import convert_image_dtype\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def extract_features(model, image_dir, k=80):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    feats = model(images)\n",
        "    feats = feats.detach().cpu().numpy()\n",
        "    patcho = image_dir\n",
        "    # Take average and get final prediction\n",
        "    # feats = feats.mean(0)\n",
        "\n",
        "    ###### Add line below to group all patch features / image\n",
        "    #feats = np.sort(feats, 0)[::-1][:k].mean(0)\n",
        "    print(feats.shape)\n",
        "    return feats\n",
        "\n",
        "# Validation dir\n",
        "\n",
        "                     \n",
        "val_dir =  \"C:/Users/meada/OneDrive/Desktop/Podocarpites-Copy/Stacks/\"\n",
        "\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "\n",
        "# Modify output\n",
        "model.fc = nn.Identity()\n",
        "model.eval()\n",
        "\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "all_image_dirs = []\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "All_feats_shape = []\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        feats = extract_features(model, image_dir)\n",
        "        features.append(feats)\n",
        "        All_feats_shape.append(feats.shape)\n",
        "### Replace by np.stack (vs concatenate) to group all patch features/image\n",
        "features = np.concatenate(features, 0)\n",
        "features_Stacks_Indiv = features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "199d2372",
      "metadata": {
        "id": "199d2372"
      },
      "outputs": [],
      "source": [
        "# ResNeXt101, pretrained on imagenet-1K dataset\n",
        "\n",
        "\n",
        "PATH = \"C:/Users/meada/OneDrive/Desktop/Podocarpus_Images_Split4.pt\"\n",
        "\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 30); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce202fca",
      "metadata": {
        "id": "ce202fca",
        "outputId": "b3ffdc7f-6940-421d-99a7-bb86aa934c7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n"
          ]
        }
      ],
      "source": [
        "from torchvision.transforms.transforms import ConvertImageDtype\n",
        "from torchvision.transforms.functional import convert_image_dtype\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def extract_features(model, image_dir, k=80):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    feats = model(images)\n",
        "    feats = feats.detach().cpu().numpy()\n",
        "    patcho = image_dir\n",
        "\n",
        "    # Take average and get final prediction\n",
        "    # feats = feats.mean(0)\n",
        "\n",
        "    ###### Add line below to group all patch features / image\n",
        "    feats = np.sort(feats, 0)[::-1][:k].mean(0)\n",
        "    print(feats.shape)\n",
        "    return feats\n",
        "   \n",
        "\n",
        "# Validation dir\n",
        "\n",
        "\n",
        "val_dir =  \"C:/Users/meada/OneDrive/Desktop/Podocarpites-Copy/Images/\"\n",
        "\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "\n",
        "# Modify output\n",
        "model.fc = nn.Identity()\n",
        "model.eval()\n",
        "\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "all_image_dirs = []\n",
        "features = []\n",
        "labels = []\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        feats = extract_features(model, image_dir)\n",
        "        features.append(feats)\n",
        "### Replace by np.stack (vs concatenate) to group all patch features/image\n",
        "features = np.stack(features, 0)\n",
        "\n",
        "features_Images_Average = features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85af951b",
      "metadata": {
        "id": "85af951b"
      },
      "outputs": [],
      "source": [
        "RepeatedImages = []\n",
        "for i in range(len(All_feats_shape)):\n",
        "    repeatedImage = torch.tensor(features_Images_Average[i]).repeat(All_feats_shape[i][0],1)\n",
        "    RepeatedImages.append(repeatedImage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc4ce44d",
      "metadata": {
        "id": "dc4ce44d"
      },
      "outputs": [],
      "source": [
        "# ResNeXt101, pretrained on imagenet-1K dataset\n",
        "\n",
        "\n",
        "PATH = \"C:/Users/meada/OneDrive/Desktop/Podocarpus_Patches_Split4.pt\"\n",
        "\n",
        "\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 30); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54539ae7",
      "metadata": {
        "id": "54539ae7",
        "outputId": "68c1bee3-8768-4334-ce58-eb9fadbd3dea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n",
            "(2048,)\n"
          ]
        }
      ],
      "source": [
        "from torchvision.transforms.transforms import ConvertImageDtype\n",
        "from torchvision.transforms.functional import convert_image_dtype\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def extract_features(model, image_dir, k=80):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    feats = model(images)\n",
        "    feats = feats.detach().cpu().numpy()\n",
        "    patcho = image_dir\n",
        "\n",
        "    # Take average and get final prediction\n",
        "    # feats = feats.mean(0)\n",
        "\n",
        "    ###### Add line below to group all patch features / image\n",
        "    feats = np.sort(feats, 0)[::-1][:k].mean(0)\n",
        "    print(feats.shape)\n",
        "    return feats\n",
        "   \n",
        "\n",
        "# Validation dir\n",
        "\n",
        "\n",
        "val_dir =  \"C:/Users/meada/OneDrive/Desktop/Podocarpites-Copy/Patches/\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "\n",
        "# Modify output\n",
        "model.fc = nn.Identity()\n",
        "model.eval()\n",
        "\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "all_image_dirs = []\n",
        "features = []\n",
        "labels = []\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        feats = extract_features(model, image_dir)\n",
        "        features.append(feats)\n",
        "### Replace by np.stack (vs concatenate) to group all patch features/image\n",
        "features = np.stack(features, 0)\n",
        "\n",
        "features_Patches_Average = features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "058ae76f",
      "metadata": {
        "id": "058ae76f"
      },
      "outputs": [],
      "source": [
        "RepeatedPatches = []\n",
        "for i in range(len(All_feats_shape)):\n",
        "    repeatedPatch = torch.tensor(features_Patches_Average[i]).repeat(All_feats_shape[i][0],1)\n",
        "    RepeatedPatches.append(repeatedPatch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace5a09e",
      "metadata": {
        "id": "ace5a09e"
      },
      "source": [
        "### Concatenate All individual cross-sectional image features (along with the fixed MIP and patch features) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6a959b0",
      "metadata": {
        "id": "c6a959b0",
        "outputId": "4b4fdefc-d564-444b-80ff-2fa6c5057c41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\meada\\AppData\\Local\\Temp/ipykernel_20584/3742222681.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  JoinedTensorImages = torch.tensor(torch.cat(RepeatedImages))\n",
            "C:\\Users\\meada\\AppData\\Local\\Temp/ipykernel_20584/3742222681.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  JoinedTensorPatches = torch.tensor(torch.cat(RepeatedPatches))\n"
          ]
        }
      ],
      "source": [
        "JoinedTensorImages = torch.tensor(torch.cat(RepeatedImages))\n",
        "JoinedTensorPatches = torch.tensor(torch.cat(RepeatedPatches))\n",
        "features_Stacks_Indiv_tensor = torch.tensor(features_Stacks_Indiv)\n",
        "Concatenated_Modalities_Informative_Stacks = torch.cat((features_Stacks_Indiv_tensor,JoinedTensorImages,JoinedTensorPatches),1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99240c44",
      "metadata": {
        "id": "99240c44"
      },
      "source": [
        "# Extract individual PATCH features while keeping Cross-sectional and MIP Features Fixed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abb93a60",
      "metadata": {
        "id": "abb93a60"
      },
      "outputs": [],
      "source": [
        "# ResNeXt101, pretrained on imagenet-1K dataset\n",
        "\n",
        "\n",
        "PATH = \"C:/Users/meada/OneDrive/Desktop/Podocarpus_Patches_Split4.pt\"\n",
        "    \n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 30); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06f23c0f",
      "metadata": {
        "id": "06f23c0f"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.transforms import ConvertImageDtype\n",
        "from torchvision.transforms.functional import convert_image_dtype\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def extract_features(model, image_dir, k=80):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    feats = model(images)\n",
        "    feats = feats.detach().cpu().numpy()\n",
        "    patcho = image_dir\n",
        "\n",
        "    # Take average and get final prediction\n",
        "    # feats = feats.mean(0)\n",
        "\n",
        "    ###### Add line below to group all patch features / image\n",
        "    #feats = np.sort(feats, 0)[::-1][:k].mean(0)\n",
        "    print(feats.shape)\n",
        "    return feats\n",
        "   \n",
        "\n",
        "# Validation dir\n",
        "\n",
        "val_dir =  \"C:/Users/meada/OneDrive/Desktop/Podocarpites-Copy/Patches/\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "\n",
        "# Modify output\n",
        "model.fc = nn.Identity()\n",
        "model.eval()\n",
        "\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "all_image_dirs = []\n",
        "features = []\n",
        "labels = []\n",
        "All_feats_shape = []\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        feats = extract_features(model, image_dir)\n",
        "        features.append(feats)\n",
        "        All_feats_shape.append(feats.shape)\n",
        "\n",
        "### Replace by np.stack (vs concatenate) to group all patch features/image\n",
        "features = np.concatenate(features, 0)\n",
        "\n",
        "features_Patches_Indiv = features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "345ec2d2",
      "metadata": {
        "id": "345ec2d2"
      },
      "outputs": [],
      "source": [
        "# Load na誰ve classifier (Cross-sectional Images) \n",
        "# ResNeXt101, pretrained on imagenet-1K dataset\n",
        "\n",
        "\n",
        "PATH = \"C:/Users/meada/OneDrive/Desktop/Podocarpus_Stacks_Split4.pt\"\n",
        "\n",
        "\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 30); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a9b1606",
      "metadata": {
        "id": "6a9b1606"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.transforms import ConvertImageDtype\n",
        "from torchvision.transforms.functional import convert_image_dtype\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def extract_features(model, image_dir, k=80):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    feats = model(images)\n",
        "    feats = feats.detach().cpu().numpy()\n",
        "    patcho = image_dir\n",
        "    # Take average and get final prediction\n",
        "    # feats = feats.mean(0)\n",
        "\n",
        "    ###### Add line below to group all patch features / image\n",
        "    feats = np.sort(feats, 0)[::-1][:k].mean(0)\n",
        "    print(feats.shape)\n",
        "    return feats\n",
        "\n",
        "# Validation dir\n",
        "\n",
        "\n",
        "val_dir =  \"C:/Users/meada/OneDrive/Desktop/Podocarpites-Copy/Stacks/\"\n",
        "\n",
        "\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "\n",
        "# Modify output\n",
        "model.fc = nn.Identity()\n",
        "model.eval()\n",
        "\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "all_image_dirs = []\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        feats = extract_features(model, image_dir)\n",
        "        features.append(feats)\n",
        "### Replace by np.stack (vs concatenate) to group all patch features/image\n",
        "features = np.stack(features, 0)\n",
        "features_Stacks_Average = features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "325faaae",
      "metadata": {
        "id": "325faaae"
      },
      "outputs": [],
      "source": [
        "RepeatedStacks = []\n",
        "for i in range(len(All_feats_shape)):\n",
        "    repeatedStack = torch.tensor(features_Stacks_Average[i]).repeat(All_feats_shape[i][0],1)\n",
        "    RepeatedStacks.append(repeatedStack)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93774e3b",
      "metadata": {
        "id": "93774e3b"
      },
      "outputs": [],
      "source": [
        "# ResNeXt101, pretrained on imagenet-1K dataset\n",
        "\n",
        "\n",
        "PATH = \"C:/Users/meada/OneDrive/Desktop/Podocarpus_Images_Split4.pt\"\n",
        "\n",
        "\n",
        "\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 30); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b15b6b4",
      "metadata": {
        "id": "1b15b6b4"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms.transforms import ConvertImageDtype\n",
        "from torchvision.transforms.functional import convert_image_dtype\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def extract_features(model, image_dir, k=80):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    feats = model(images)\n",
        "    feats = feats.detach().cpu().numpy()\n",
        "    patcho = image_dir\n",
        "\n",
        "    # Take average and get final prediction\n",
        "    # feats = feats.mean(0)\n",
        "\n",
        "    ###### Add line below to group all patch features / image\n",
        "    feats = np.sort(feats, 0)[::-1][:k].mean(0)\n",
        "    print(feats.shape)\n",
        "    return feats\n",
        "   \n",
        "\n",
        "# Validation dir\n",
        "\n",
        "\n",
        "val_dir =  \"C:/Users/meada/OneDrive/Desktop/Podocarpites-Copy/Images/\"\n",
        "\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "\n",
        "# Modify output\n",
        "model.fc = nn.Identity()\n",
        "model.eval()\n",
        "\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "all_image_dirs = []\n",
        "features = []\n",
        "labels = []\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        feats = extract_features(model, image_dir)\n",
        "        features.append(feats)\n",
        "### Replace by np.stack (vs concatenate) to group all patch features/image\n",
        "features = np.stack(features, 0)\n",
        "\n",
        "features_Images_Average = features "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4924a91c",
      "metadata": {
        "id": "4924a91c"
      },
      "outputs": [],
      "source": [
        "RepeatedImages = []\n",
        "for i in range(len(All_feats_shape)):\n",
        "    repeatedImage = torch.tensor(features_Images_Average[i]).repeat(All_feats_shape[i][0],1)\n",
        "    RepeatedImages.append(repeatedImage)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31372c9a",
      "metadata": {
        "id": "31372c9a"
      },
      "source": [
        "### Concatenate All individual PATCH features (along with the fixed Cross-sectional and MIP features) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f006be6",
      "metadata": {
        "id": "5f006be6"
      },
      "outputs": [],
      "source": [
        "JoinedTensorStacks = torch.tensor(torch.cat(RepeatedStacks))\n",
        "JoinedTensorImages = torch.tensor(torch.cat(RepeatedImages))\n",
        "features_Patches_Indiv_tensor = torch.tensor(features_Patches_Indiv)\n",
        "Concatenated_Modalities_Informative_Patches = torch.cat((JoinedTensorStacks,JoinedTensorImages,features_Patches_Indiv_tensor),1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb80c326",
      "metadata": {
        "id": "fb80c326"
      },
      "outputs": [],
      "source": [
        "Concatenated_Modalities_Informative_Patches.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1822116e",
      "metadata": {
        "id": "1822116e"
      },
      "source": [
        "# Extract Known Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b5113fa",
      "metadata": {
        "id": "6b5113fa"
      },
      "source": [
        "## Cross-Sectional Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3689a01a",
      "metadata": {
        "id": "3689a01a"
      },
      "outputs": [],
      "source": [
        "# Load na誰ve classifier (Cross-sectional Images) \n",
        "# ResNeXt101, pretrained on imagenet-1K dataset\n",
        "\n",
        "PATH = \"C:/Users/meada/OneDrive/Desktop/Podocarpus_Stacks_Split4.pt\"\n",
        "\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 30); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);\n",
        "\n",
        "from torchvision.transforms.transforms import ConvertImageDtype\n",
        "from torchvision.transforms.functional import convert_image_dtype\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def extract_features(model, image_dir, k=80):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    feats = model(images)\n",
        "    feats = feats.detach().cpu().numpy()\n",
        "    patcho = image_dir\n",
        "    # Take average and get final prediction\n",
        "    # feats = feats.mean(0)\n",
        "\n",
        "    ###### Add line below to group all patch features / image\n",
        "    feats = np.sort(feats, 0)[::-1][:k].mean(0)\n",
        "    print(feats.shape)\n",
        "    return feats\n",
        "\n",
        "# Validation dir\n",
        "\n",
        "val_dir = \"C:/Users/meada/OneDrive/Desktop/Stacks_All_Podocarpus_All/\"\n",
        "\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "\n",
        "# Modify output\n",
        "model.fc = nn.Identity()\n",
        "model.eval()\n",
        "\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "all_image_dirs = []\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "All_feats_shape = []\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        feats = extract_features(model, image_dir)\n",
        "        features.append(feats)\n",
        "        All_feats_shape.append(feats.shape)\n",
        "### Replace by np.stack (vs concatenate) to group all patch features/image\n",
        "features = np.stack(features, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6185724a",
      "metadata": {
        "id": "6185724a"
      },
      "outputs": [],
      "source": [
        "# Concatenate Cross-Sectional Image Directories and FEATURES, and get sorted feature vectors\n",
        "import numpy as np \n",
        "Dirs_and_Features = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_feature = features[i]\n",
        "  Dirs_and_Features[array_dir] = array_feature\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "    \n",
        "Stack_Features_Sorted = []\n",
        "for key in sorted(Dirs_and_Features.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Stack_Features_Sorted.append(Dirs_and_Features[key])\n",
        "\n",
        "labels.sort()\n",
        "Stack_Labels = labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c01cd6",
      "metadata": {
        "id": "95c01cd6"
      },
      "source": [
        "## MIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4403c62",
      "metadata": {
        "id": "e4403c62"
      },
      "outputs": [],
      "source": [
        "# ResNeXt101, pretrained on imagenet-1K dataset\n",
        "\n",
        "\n",
        "PATH = \"C:/Users/meada/OneDrive/Desktop/Podocarpus_Images_Split4.pt\"\n",
        "\n",
        "\n",
        "\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 30); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);\n",
        "\n",
        "from torchvision.transforms.transforms import ConvertImageDtype\n",
        "from torchvision.transforms.functional import convert_image_dtype\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def extract_features(model, image_dir, k=80):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    feats = model(images)\n",
        "    feats = feats.detach().cpu().numpy()\n",
        "    patcho = image_dir\n",
        "    # Take average and get final prediction\n",
        "    # feats = feats.mean(0)\n",
        "\n",
        "    ###### Add line below to group all patch features / image\n",
        "    feats = np.sort(feats, 0)[::-1][:k].mean(0)\n",
        "    print(feats.shape)\n",
        "    return feats\n",
        "\n",
        "# Validation dir\n",
        "\n",
        "\n",
        "val_dir = \"C:/Users/meada/OneDrive/Desktop/Images_All_Podocarpus_All/\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "\n",
        "# Modify output\n",
        "model.fc = nn.Identity()\n",
        "model.eval()\n",
        "\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "all_image_dirs = []\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "All_feats_shape = []\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        feats = extract_features(model, image_dir)\n",
        "        features.append(feats)\n",
        "        All_feats_shape.append(feats.shape)\n",
        "### Replace by np.stack (vs concatenate) to group all patch features/image\n",
        "features = np.stack(features, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdfc6f55",
      "metadata": {
        "id": "cdfc6f55"
      },
      "outputs": [],
      "source": [
        "# Concatenate Image (whole images) Directories and FEATURES, and get sorted feature vectors\n",
        "import numpy as np \n",
        "Dirs_and_Features = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_feature = features[i]\n",
        "  Dirs_and_Features[array_dir] = array_feature\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "\n",
        "Image_Features_Sorted = []\n",
        "for key in sorted(Dirs_and_Features.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "    labels.sort()\n",
        "    Image_Labels = labels\n",
        "    Image_Features_Sorted.append(Dirs_and_Features[key])\n",
        "\n",
        "labels.sort()\n",
        "Image_Labels = labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cc348d4",
      "metadata": {
        "id": "6cc348d4"
      },
      "outputs": [],
      "source": [
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83d4e5ab",
      "metadata": {
        "id": "83d4e5ab"
      },
      "source": [
        "## Patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c6e40e",
      "metadata": {
        "id": "15c6e40e"
      },
      "outputs": [],
      "source": [
        "# ResNeXt101, pretrained on imagenet-1K dataset\n",
        "\n",
        "PATH = \"C:/Users/meada/OneDrive/Desktop/Podocarpus_Patches_Split4.pt\"\n",
        "\n",
        "\n",
        "\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 30); \n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);\n",
        "\n",
        "from torchvision.transforms.transforms import ConvertImageDtype\n",
        "from torchvision.transforms.functional import convert_image_dtype\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "def extract_features(model, image_dir, k=80):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    feats = model(images)\n",
        "    feats = feats.detach().cpu().numpy()\n",
        "    patcho = image_dir\n",
        "    # Take average and get final prediction\n",
        "    # feats = feats.mean(0)\n",
        "\n",
        "    ###### Add line below to group all patch features / image\n",
        "    feats = np.sort(feats, 0)[::-1][:k].mean(0)\n",
        "    print(feats.shape)\n",
        "    return feats\n",
        "\n",
        "# Validation dir\n",
        "\n",
        "val_dir = \"C:/Users/meada/OneDrive/Desktop/Patches_All_Podocarpus_All/\"\n",
        "\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "\n",
        "# Modify output\n",
        "model.fc = nn.Identity()\n",
        "model.eval()\n",
        "\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "all_image_dirs = []\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "All_feats_shape = []\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)  \n",
        "        feats = extract_features(model, image_dir)\n",
        "        features.append(feats)\n",
        "        All_feats_shape.append(feats.shape)\n",
        "### Replace by np.stack (vs concatenate) to group all patch features/image\n",
        "features = np.stack(features, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f743707b",
      "metadata": {
        "id": "f743707b"
      },
      "outputs": [],
      "source": [
        "# Concatenate Patch Directories and FEATURES, and get sorted feature vectors\n",
        "import numpy as np \n",
        "Dirs_and_Features = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_feature = features[i]\n",
        "  Dirs_and_Features[array_dir] = array_feature\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "    \n",
        "Patch_Features_Sorted = []\n",
        "for key in sorted(Dirs_and_Features.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Patch_Features_Sorted.append(Dirs_and_Features[key])\n",
        "\n",
        "labels.sort()\n",
        "Patch_Labels = label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23b66f39",
      "metadata": {
        "id": "23b66f39"
      },
      "outputs": [],
      "source": [
        "Concatenated_Features_Three_Modalities = torch.cat((torch.tensor(Stack_Features_Sorted), torch.tensor(Image_Features_Sorted), torch.tensor(Patch_Features_Sorted)), dim =1)\n",
        "Concatenated_Features_Three_Modalities.shape\n",
        "Concatenated_Features_Three_Modalities_Known = Concatenated_Features_Three_Modalities\n",
        "Y = Image_Labels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69cd951b",
      "metadata": {
        "id": "69cd951b",
        "outputId": "7d54c0d5-cd44-4b6c-a497-ab7221dc7956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([309, 6144])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\meada\\AppData\\Local\\Temp/ipykernel_20584/2310276081.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_train = torch.tensor(Concatenated_Features_Three_Modalities_Known)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([30, 30])"
            ]
          },
          "execution_count": 621,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Podocarpites\n",
        " \n",
        "Y = labels\n",
        "\n",
        "C = np.array([\n",
        "[0,99.79615,63.13772,99.79615,63.13772,99.79615,99.79615,63.13772,99.79615,30.05496,63.13772,30.05496,99.79615,99.79615,63.13772,63.13772,99.79615,30.05496,99.79615,63.13772,63.13772,63.13772,99.79615,99.79615,72.74426,63.13772,99.79615,99.79615,12.58331,63.13772],\n",
        "[99.79615,0,99.79615,36.20694,99.79615,65.49675,49.15364,99.79615,65.49675,99.79615,99.79615,99.79615,49.15364,28.1985,99.79615,99.79615,42.4938,99.79615,49.15364,99.79615,99.79615,99.79615,28.1985,49.15364,99.79615,99.79615,65.49675,49.15364,99.79615,99.79615],\n",
        "[63.13772,99.79615,0,99.79615,15.93869,99.79615,99.79615,52.67842,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,33.41821,52.67842,99.79615,63.13772,99.79615,58.99733,25.55484,58.99733,99.79615,99.79615,72.74426,15.93869,99.79615,99.79615,63.13772,25.55484],\n",
        "[99.79615,36.20694,99.79615,0,99.79615,65.49675,49.15363,99.79615,65.49675,99.79615,99.79615,99.79615,49.15363,36.20694,99.79615,99.79615,42.4938,99.79615,49.15363,99.79615,99.79615,99.79615,36.20694,49.15363,99.79615,99.79615,65.49675,49.15363,99.79615,99.79615],\n",
        "[63.13772,99.79615,15.93869,99.79615,0,99.79615,99.79615,52.67842,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,33.41821,52.67842,99.79615,63.13772,99.79615,58.99733,25.55484,58.99733,99.79615,99.79615,72.74426,12.69516,99.79615,99.79615,63.13772,25.55484],\n",
        "[99.79615,65.49675,99.79615,65.49675,99.79615,0,65.49675,99.79615,32.59701,99.79615,99.79615,99.79615,65.49675,65.49675,99.79615,99.79615,65.49675,99.79615,65.49675,99.79615,99.79615,99.79615,65.49675,65.49675,99.79615,99.79615,32.59701,65.49675,99.79615,99.79615],\n",
        "[99.79615,49.15364,99.79615,49.15363,99.79615,65.49675,0,99.79615,65.49675,99.79615,99.79615,99.79615,28.27449,49.15363,99.79615,99.79615,49.15364,99.79615,28.27449,99.79615,99.79615,99.79615,49.15363,10.13461,99.79615,99.79615,65.49675,28.27449,99.79615,99.79615],\n",
        "[63.13772,99.79615,52.67842,99.79615,52.67842,99.79615,99.79615,0,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,52.67842,12.04249,99.79615,63.13772,99.79615,58.99733,52.67842,58.99733,99.79615,99.79615,72.74426,52.67842,99.79615,99.79615,63.13772,52.67842],\n",
        "[99.79615,65.49675,99.79615,65.49675,99.79615,32.59701,65.49675,99.79615,0,99.79615,99.79615,99.79615,65.49675,65.49675,99.79615,99.79615,65.49675,99.79615,65.49675,99.79615,99.79615,99.79615,65.49675,65.49675,99.79615,99.79615,2.84746,65.49675,99.79615,99.79615],\n",
        "[30.05496,99.79615,63.13772,99.79615,63.13772,99.79615,99.79615,63.13772,99.79615,0,63.13772,7.36035,99.79615,99.79615,63.13772,63.13772,99.79615,14.47632,99.79615,63.13772,63.13772,63.13772,99.79615,99.79615,72.74426,63.13772,99.79615,99.79615,30.05496,63.13772],\n",
        "[63.13772,99.79615,58.99733,99.79615,58.99733,99.79615,99.79615,58.99733,99.79615,63.13772,0,63.13772,99.79615,99.79615,58.99733,58.99733,99.79615,63.13772,99.79615,51.13068,58.99733,23.76483,99.79615,99.79615,72.74426,58.99733,99.79615,99.79615,63.13772,58.99733],\n",
        "[30.05496,99.79615,63.13772,99.79615,63.13772,99.79615,99.79615,63.13772,99.79615,7.36035,63.13772,0,99.79615,99.79615,63.13772,63.13772,99.79615,14.47632,99.79615,63.13772,63.13772,63.13772,99.79615,99.79615,72.74426,63.13772,99.79615,99.79615,30.05496,63.13772],\n",
        "[99.79615,49.15364,99.79615,49.15363,99.79615,65.49675,28.27449,99.79615,65.49675,99.79615,99.79615,99.79615,0,49.15363,99.79615,99.79615,49.15364,99.79615,20.10967,99.79615,99.79615,99.79615,49.15363,28.27449,99.79615,99.79615,65.49675,7.82913,99.79615,99.79615],\n",
        "[99.79615,28.1985,99.79615,36.20694,99.79615,65.49675,49.15363,99.79615,65.49675,99.79615,99.79615,99.79615,49.15363,0,99.79615,99.79615,42.4938,99.79615,49.15363,99.79615,99.79615,99.79615,5.34205,49.15363,99.79615,99.79615,65.49675,49.15363,99.79615,99.79615],\n",
        "[63.13772,99.79615,33.41821,99.79615,33.41821,99.79615,99.79615,52.67842,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,0,52.67842,99.79615,63.13772,99.79615,58.99733,33.41822,58.99733,99.79615,99.79615,72.74426,33.41821,99.79615,99.79615,63.13772,33.41821],\n",
        "[63.13772,99.79615,52.67842,99.79615,52.67842,99.79615,99.79615,12.04249,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,52.67842,0,99.79615,63.13772,99.79615,58.99733,52.67842,58.99733,99.79615,99.79615,72.74426,52.67842,99.79615,99.79615,63.13772,52.67842],\n",
        "[99.79615,42.4938,99.79615,42.4938,99.79615,65.49675,49.15364,99.79615,65.49675,99.79615,99.79615,99.79615,49.15364,42.4938,99.79615,99.79615,0,99.79615,49.15364,99.79615,99.79615,99.79615,42.4938,49.15364,99.79615,99.79615,65.49675,49.15364,99.79615,99.79615],\n",
        "[30.05496,99.79615,63.13772,99.79615,63.13772,99.79615,99.79615,63.13772,99.79615,14.47632,63.13772,14.47632,99.79615,99.79615,63.13772,63.13772,99.79615,0,99.79615,63.13772,63.13772,63.13772,99.79615,99.79615,72.74426,63.13772,99.79615,99.79615,30.05496,63.13772],\n",
        "[99.79615,49.15364,99.79615,49.15363,99.79615,65.49675,28.27449,99.79615,65.49675,99.79615,99.79615,99.79615,20.10967,49.15363,99.79615,99.79615,49.15364,99.79615,0,99.79615,99.79615,99.79615,49.15363,28.27449,99.79615,99.79615,65.49675,20.10967,99.79615,99.79615],\n",
        "[63.13772,99.79615,58.99733,99.79615,58.99733,99.79615,99.79615,58.99733,99.79615,63.13772,51.13068,63.13772,99.79615,99.79615,58.99733,58.99733,99.79615,63.13772,99.79615,0,58.99733,51.13068,99.79615,99.79615,72.74426,58.99733,99.79615,99.79615,63.13772,58.99733],\n",
        "[63.13772,99.79615,25.55484,99.79615,25.55484,99.79615,99.79615,52.67842,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,33.41822,52.67842,99.79615,63.13772,99.79615,58.99733,0,58.99733,99.79615,99.79615,72.74426,25.55484,99.79615,99.79615,63.13772,22.27456],\n",
        "[63.13772,99.79615,58.99733,99.79615,58.99733,99.79615,99.79615,58.99733,99.79615,63.13772,23.76483,63.13772,99.79615,99.79615,58.99733,58.99733,99.79615,63.13772,99.79615,51.13068,58.99733,0,99.79615,99.79615,72.74426,58.99733,99.79615,99.79615,63.13772,58.99733],\n",
        "[99.79615,28.1985,99.79615,36.20694,99.79615,65.49675,49.15363,99.79615,65.49675,99.79615,99.79615,99.79615,49.15363,5.34205,99.79615,99.79615,42.4938,99.79615,49.15363,99.79615,99.79615,99.79615,0,49.15363,99.79615,99.79615,65.49675,49.15363,99.79615,99.79615],\n",
        "[99.79615,49.15364,99.79615,49.15363,99.79615,65.49675,10.13461,99.79615,65.49675,99.79615,99.79615,99.79615,28.27449,49.15363,99.79615,99.79615,49.15364,99.79615,28.27449,99.79615,99.79615,99.79615,49.15363,0,99.79615,99.79615,65.49675,28.27449,99.79615,99.79615],\n",
        "[72.74426,99.79615,72.74426,99.79615,72.74426,99.79615,99.79615,72.74426,99.79615,72.74426,72.74426,72.74426,99.79615,99.79615,72.74426,72.74426,99.79615,72.74426,99.79615,72.74426,72.74426,72.74426,99.79615,99.79615,0,72.74426,99.79615,99.79615,72.74426,72.74426],\n",
        "[63.13772,99.79615,15.93869,99.79615,12.69516,99.79615,99.79615,52.67842,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,33.41821,52.67842,99.79615,63.13772,99.79615,58.99733,25.55484,58.99733,99.79615,99.79615,72.74426,0,99.79615,99.79615,63.13772,25.55484],\n",
        "[99.79615,65.49675,99.79615,65.49675,99.79615,32.59701,65.49675,99.79615,2.84746,99.79615,99.79615,99.79615,65.49675,65.49675,99.79615,99.79615,65.49675,99.79615,65.49675,99.79615,99.79615,99.79615,65.49675,65.49675,99.79615,99.79615,0,65.49675,99.79615,99.79615],\n",
        "[99.79615,49.15364,99.79615,49.15363,99.79615,65.49675,28.27449,99.79615,65.49675,99.79615,99.79615,99.79615,7.82913,49.15363,99.79615,99.79615,49.15364,99.79615,20.10967,99.79615,99.79615,99.79615,49.15363,28.27449,99.79615,99.79615,65.49675,0,99.79615,99.79615],\n",
        "[12.58331,99.79615,63.13772,99.79615,63.13772,99.79615,99.79615,63.13772,99.79615,30.05496,63.13772,30.05496,99.79615,99.79615,63.13772,63.13772,99.79615,30.05496,99.79615,63.13772,63.13772,63.13772,99.79615,99.79615,72.74426,63.13772,99.79615,99.79615,0,63.13772],\n",
        "[63.13772,99.79615,25.55484,99.79615,25.55484,99.79615,99.79615,52.67842,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,33.41821,52.67842,99.79615,63.13772,99.79615,58.99733,22.27456,58.99733,99.79615,99.79615,72.74426,25.55484,99.79615,99.79615,63.13772,0]\n",
        "])\n",
        "\n",
        "\n",
        "C = C/C.max()\n",
        "C = torch.from_numpy(C)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch \n",
        "\n",
        "\n",
        "Genera = [\"$\\it{P. acutifolius}$\",\"$\\it{P. archboldii}$\",\"$\\it{P. brasiliensis}$\",\"$\\it{P. brassii}$\",\"$\\it{P. coriaceus}$\",\"$\\it{P. drouynianus}$\",\"$\\it{P. elatus}$\",\"$\\it{P. elongatus}$\",\"$\\it{P. glaucus}$\",\"$\\it{P. gnidiodes}$\",\"$\\it{P. lambertii}$\",\"$\\it{P. lawrencei}$\",\"$\\it{P. lucienii}$\",\"$\\it{P. macrophyllus}$\",\"$\\it{P. matudae}$\",\"$\\it{P. milanjianus}$\",\"$\\it{P. neriifolius}$\",\"$\\it{P. nivalis}$\",\"$\\it{P. novae-caledoniae}$\",\"$\\it{P. nubigenus}$\",\"$\\it{P. oleifolius}$\",\"$\\it{P. parlatorei}$\",\"$\\it{P. pilgeri}$\",\"$\\it{P. polystachyus}$\",\"$\\it{P. salignus}$\",\"$\\it{P. sellowii}$\",\"$\\it{P. spinulosus}$\",\"$\\it{P. sylvestris}$\",\"$\\it{P. totara}$\",\"$\\it{P. urbanii}$\"];\n",
        "\n",
        "\n",
        "GenusList = []\n",
        "for i in range(len(Y)):\n",
        " GenusNumber = Y[i]\n",
        " GenusName = Genera[GenusNumber]\n",
        " GenusList.append(GenusName)\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "X_train = torch.tensor(Concatenated_Features_Three_Modalities_Known)\n",
        "print(X_train.shape)\n",
        "\n",
        "Y_tens = torch.tensor(Y)\n",
        "C.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4a8e30",
      "metadata": {
        "id": "fa4a8e30"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38143c53",
      "metadata": {
        "id": "38143c53"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81f0b777",
      "metadata": {
        "id": "81f0b777"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a568ab8",
      "metadata": {
        "id": "3a568ab8",
        "outputId": "3b5b9884-4146-4490-c46e-86d3294cf77c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([309, 309])"
            ]
          },
          "execution_count": 622,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get image-wise distance matrix from class-wise distance matrix\n",
        "\n",
        "repeated_rows = []\n",
        "\n",
        "for i in range(len(C)):\n",
        "  a = torch.Tensor(C[i].float())\n",
        "  a_repeat = a.repeat(torch.bincount(Y_tens).numpy()[i],1)\n",
        "  repeated_rows.append(a_repeat)\n",
        "  repeated_rows\n",
        "  repeated_rows_cat = torch.cat(repeated_rows,0)\n",
        "  repeated_rows_cat_transpose = torch.transpose(repeated_rows_cat,0,1)\n",
        "  repeated_rows_transpose = []\n",
        "  \n",
        "  for j in range(len(repeated_rows_cat_transpose)):\n",
        "    b = torch.Tensor(repeated_rows_cat_transpose[j])\n",
        "    b_repeat = b.repeat(torch.bincount(Y_tens).numpy()[j],1)\n",
        "    repeated_rows_transpose.append(b_repeat)\n",
        "    \n",
        "    Image_wise_matrix = torch.cat(repeated_rows_transpose)\n",
        "Image_wise_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6ada48",
      "metadata": {
        "id": "5a6ada48",
        "outputId": "b9091c4e-5071-45c5-d2b8-ad8f2b3802c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\meada\\AppData\\Local\\Temp/ipykernel_20584/3840597558.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(X_train)\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(X_train)\n",
        "y = Image_wise_matrix\n",
        "#labels = np.array(y)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "labels = y;\n",
        "labels.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944fd6bb",
      "metadata": {
        "id": "944fd6bb"
      },
      "outputs": [],
      "source": [
        "# Cosine Distance-Based Loss Function \n",
        "\n",
        "def sim_matrix(a, b, eps=1e-8):\n",
        "    \"\"\"\n",
        "    added eps for numerical stability\n",
        "    \"\"\"\n",
        "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
        "    a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
        "    b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
        "    sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
        "    #sim_mt = a_norm@b_norm.T \n",
        "    return sim_mt\n",
        "\n",
        "def compute_loss_cosine(output, label = Image_wise_matrix):\n",
        "    distance_mat = 1 - sim_matrix(output, output, eps = 1e-8)\n",
        "    #distance_mat = distance_mat/distance_mat.max()\n",
        "    return torch.sum(torch.pow(Image_wise_matrix-distance_mat, 2))\n",
        "    #return torch.sum(torch.abs(Image_wise_matrix - distance_mat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9d7db9",
      "metadata": {
        "id": "ac9d7db9",
        "outputId": "2500aa78-3b59-4d8a-e17a-d59a612b0d05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([309, 6144])\n"
          ]
        }
      ],
      "source": [
        "# Get training data \n",
        "\n",
        "train_data = []\n",
        "for i in range(len(x)):\n",
        "   train_data.append([x[i], labels[i]])\n",
        "trainloader = torch.utils.data.DataLoader(train_data, shuffle=False, batch_size=len(y))\n",
        "i1, l1 = next(iter(trainloader))\n",
        "print(i1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a04982",
      "metadata": {
        "id": "68a04982"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# define the NN architecture\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6144 , 1024)\n",
        "\n",
        "        # Add \n",
        "\n",
        "        #self.a = torch.nn.parameter.Parameter(torch.tensor([1.], requires_grad = False))\n",
        "        #self.b = torch.nn.parameter.Parameter(torch.tensor([1.], requires_grad = False))\n",
        "        #self.c = torch.nn.parameter.Parameter(torch.tensor([1.], requires_grad = False))\n",
        "\n",
        "        self.a = nn.Parameter(torch.ones(1))\n",
        "        self.b = nn.Parameter(torch.ones(1))\n",
        "        self.c = nn.Parameter(torch.ones(1))\n",
        "\n",
        "        #torch.nn.Parameter(torch.tensor([1.], requires_grad = False))\n",
        "\n",
        "        # End \n",
        "\n",
        "        #self.dropout1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(1024 , 512)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(512 , 256)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        self.fc4 = nn.Linear(256 , 256)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 6144)\n",
        "\n",
        "        aa = F.sigmoid(self.a)\n",
        "        bb = F.sigmoid(self.b)\n",
        "        cc = F.sigmoid(self.c)\n",
        "\n",
        "        aa,bb,cc = aa/(aa+bb+cc),bb/(aa+bb+cc),cc/(aa+bb+cc)\n",
        "\n",
        "        #print(\"test place 1\")\n",
        "        # Adding a \"weight\" to each of the three modalities \n",
        "        # Allows us to look at their individual contributions during training. \n",
        "        \n",
        "        x1 = aa*x[:, 0:2048]\n",
        "        x2 = bb*x[:, 2048:4096]\n",
        "        x3 = cc*x[:, 4096:6144]\n",
        "        xx = torch.cat([x1,x2,x3],1)\n",
        "        #print(\"test place 2\")\n",
        "        x = F.relu(self.fc1(xx))  \n",
        "        #x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.dropout3(x)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dd1b3d6",
      "metadata": {
        "id": "5dd1b3d6"
      },
      "outputs": [],
      "source": [
        "labels.to(device);\n",
        "X_train = X_train.to(device);\n",
        "x = x.to(device);\n",
        "y = y.to(device);\n",
        "Image_wise_matrix = Image_wise_matrix.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07f739e5",
      "metadata": {
        "id": "07f739e5",
        "outputId": "2ad101e3-89ea-44a3-fd59-fc6bc9781794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss at epoch 0: 11515.496094 \n",
            "loss at epoch 20: 8704.860352 \n",
            "loss at epoch 40: 8539.961914 \n",
            "loss at epoch 60: 8260.587891 \n",
            "loss at epoch 80: 6355.276855 \n",
            "loss at epoch 100: 3898.250000 \n",
            "loss at epoch 120: 3129.073975 \n",
            "loss at epoch 140: 2830.652100 \n",
            "loss at epoch 160: 2702.809570 \n",
            "loss at epoch 180: 2634.362305 \n",
            "loss at epoch 200: 2511.821289 \n",
            "loss at epoch 220: 2491.594482 \n",
            "loss at epoch 240: 2433.482666 \n",
            "loss at epoch 260: 2356.983887 \n",
            "loss at epoch 280: 2232.367188 \n",
            "loss at epoch 300: 2154.841064 \n",
            "loss at epoch 320: 1846.732666 \n",
            "loss at epoch 340: 1572.897217 \n",
            "loss at epoch 360: 1329.802856 \n",
            "loss at epoch 380: 1129.539673 \n",
            "loss at epoch 400: 959.626343 \n",
            "loss at epoch 420: 871.403564 \n",
            "loss at epoch 440: 824.295044 \n",
            "loss at epoch 460: 772.450928 \n",
            "loss at epoch 480: 715.871338 \n",
            "loss at epoch 500: 689.076294 \n",
            "loss at epoch 520: 604.782471 \n",
            "loss at epoch 540: 528.916321 \n",
            "loss at epoch 560: 498.471863 \n",
            "loss at epoch 580: 458.401917 \n",
            "loss at epoch 600: 435.871704 \n",
            "loss at epoch 620: 411.278931 \n",
            "loss at epoch 640: 429.439758 \n",
            "loss at epoch 660: 394.806274 \n",
            "loss at epoch 680: 394.775818 \n",
            "loss at epoch 700: 380.025269 \n",
            "loss at epoch 720: 390.342712 \n",
            "loss at epoch 740: 357.850403 \n",
            "loss at epoch 760: 358.344086 \n",
            "loss at epoch 780: 360.286194 \n",
            "loss at epoch 800: 354.051392 \n",
            "loss at epoch 820: 357.238190 \n",
            "loss at epoch 840: 332.716461 \n",
            "loss at epoch 860: 328.439117 \n",
            "loss at epoch 880: 322.963013 \n",
            "loss at epoch 900: 310.272766 \n",
            "loss at epoch 920: 319.019104 \n",
            "loss at epoch 940: 297.297333 \n",
            "loss at epoch 960: 309.541260 \n",
            "loss at epoch 980: 291.666687 \n",
            "loss at epoch 1000: 285.572571 \n",
            "loss at epoch 1020: 287.004761 \n",
            "loss at epoch 1040: 283.409180 \n",
            "loss at epoch 1060: 284.467957 \n",
            "loss at epoch 1080: 275.741516 \n",
            "loss at epoch 1100: 267.589539 \n",
            "loss at epoch 1120: 271.292786 \n",
            "loss at epoch 1140: 257.880188 \n",
            "loss at epoch 1160: 257.074127 \n",
            "loss at epoch 1180: 257.637299 \n",
            "loss at epoch 1200: 250.277283 \n",
            "loss at epoch 1220: 242.219482 \n",
            "loss at epoch 1240: 251.397812 \n",
            "loss at epoch 1260: 237.211060 \n",
            "loss at epoch 1280: 243.741028 \n",
            "loss at epoch 1300: 246.323608 \n",
            "loss at epoch 1320: 239.619339 \n",
            "loss at epoch 1340: 241.917206 \n",
            "loss at epoch 1360: 236.474701 \n",
            "loss at epoch 1380: 234.145447 \n",
            "loss at epoch 1400: 227.140442 \n",
            "loss at epoch 1420: 226.895142 \n",
            "loss at epoch 1440: 225.998413 \n",
            "loss at epoch 1460: 224.280792 \n",
            "loss at epoch 1480: 213.351837 \n",
            "loss at epoch 1500: 219.892746 \n",
            "loss at epoch 1520: 214.516846 \n",
            "loss at epoch 1540: 209.953705 \n",
            "loss at epoch 1560: 200.390411 \n",
            "loss at epoch 1580: 208.379852 \n",
            "loss at epoch 1600: 213.900391 \n",
            "loss at epoch 1620: 191.557373 \n",
            "loss at epoch 1640: 190.685822 \n",
            "loss at epoch 1660: 187.183578 \n",
            "loss at epoch 1680: 187.235703 \n",
            "loss at epoch 1700: 184.940430 \n",
            "loss at epoch 1720: 193.034653 \n",
            "loss at epoch 1740: 181.662827 \n",
            "loss at epoch 1760: 177.295929 \n",
            "loss at epoch 1780: 179.037766 \n",
            "loss at epoch 1800: 174.489532 \n",
            "loss at epoch 1820: 166.619446 \n",
            "loss at epoch 1840: 162.718292 \n",
            "loss at epoch 1860: 169.706421 \n",
            "loss at epoch 1880: 160.305222 \n",
            "loss at epoch 1900: 156.984787 \n",
            "loss at epoch 1920: 162.249573 \n",
            "loss at epoch 1940: 155.929504 \n",
            "loss at epoch 1960: 151.320755 \n",
            "loss at epoch 1980: 156.802490 \n",
            "loss at epoch 2000: 153.953552 \n",
            "loss at epoch 2020: 146.406982 \n",
            "loss at epoch 2040: 144.382355 \n",
            "loss at epoch 2060: 154.863083 \n",
            "loss at epoch 2080: 145.800934 \n",
            "loss at epoch 2100: 147.382019 \n",
            "loss at epoch 2120: 146.969421 \n",
            "loss at epoch 2140: 141.753540 \n",
            "loss at epoch 2160: 136.946609 \n",
            "loss at epoch 2180: 135.196503 \n",
            "loss at epoch 2200: 132.208801 \n",
            "loss at epoch 2220: 129.772797 \n",
            "loss at epoch 2240: 130.782150 \n",
            "loss at epoch 2260: 127.945511 \n",
            "loss at epoch 2280: 133.210236 \n",
            "loss at epoch 2300: 123.952744 \n",
            "loss at epoch 2320: 124.792480 \n",
            "loss at epoch 2340: 119.378662 \n",
            "loss at epoch 2360: 117.484253 \n",
            "loss at epoch 2380: 119.569016 \n",
            "loss at epoch 2400: 115.269257 \n",
            "loss at epoch 2420: 115.267700 \n",
            "loss at epoch 2440: 115.880875 \n",
            "loss at epoch 2460: 109.809975 \n",
            "loss at epoch 2480: 104.163139 \n",
            "loss at epoch 2500: 102.392990 \n",
            "loss at epoch 2520: 104.168594 \n",
            "loss at epoch 2540: 102.678741 \n",
            "loss at epoch 2560: 99.571213 \n",
            "loss at epoch 2580: 101.142616 \n",
            "loss at epoch 2600: 95.689911 \n",
            "loss at epoch 2620: 94.496719 \n",
            "loss at epoch 2640: 100.021210 \n",
            "loss at epoch 2660: 99.542366 \n",
            "loss at epoch 2680: 93.505135 \n",
            "loss at epoch 2700: 92.425797 \n",
            "loss at epoch 2720: 96.105843 \n",
            "loss at epoch 2740: 91.312851 \n",
            "loss at epoch 2760: 90.457748 \n",
            "loss at epoch 2780: 93.238586 \n",
            "loss at epoch 2800: 91.284203 \n",
            "loss at epoch 2820: 86.637077 \n",
            "loss at epoch 2840: 90.648148 \n",
            "loss at epoch 2860: 81.701645 \n",
            "loss at epoch 2880: 84.888077 \n",
            "loss at epoch 2900: 83.675400 \n",
            "loss at epoch 2920: 87.562744 \n",
            "loss at epoch 2940: 77.403427 \n",
            "loss at epoch 2960: 78.201477 \n",
            "loss at epoch 2980: 84.106766 \n",
            "loss at epoch 3000: 74.748077 \n",
            "loss at epoch 3020: 83.643372 \n",
            "loss at epoch 3040: 73.393997 \n",
            "loss at epoch 3060: 73.934097 \n",
            "loss at epoch 3080: 65.460762 \n",
            "loss at epoch 3100: 64.604279 \n",
            "loss at epoch 3120: 69.935349 \n",
            "loss at epoch 3140: 66.587120 \n",
            "loss at epoch 3160: 75.538406 \n",
            "loss at epoch 3180: 57.113251 \n",
            "loss at epoch 3200: 64.066162 \n",
            "loss at epoch 3220: 65.856216 \n",
            "loss at epoch 3240: 63.581104 \n",
            "loss at epoch 3260: 60.705544 \n",
            "loss at epoch 3280: 63.027882 \n",
            "loss at epoch 3300: 63.773697 \n",
            "loss at epoch 3320: 58.564369 \n",
            "loss at epoch 3340: 57.101074 \n",
            "loss at epoch 3360: 55.678898 \n",
            "loss at epoch 3380: 58.189320 \n",
            "loss at epoch 3400: 58.477676 \n",
            "loss at epoch 3420: 58.347641 \n",
            "loss at epoch 3440: 53.591888 \n",
            "loss at epoch 3460: 55.059731 \n",
            "loss at epoch 3480: 55.254215 \n",
            "loss at epoch 3500: 56.713463 \n",
            "loss at epoch 3520: 49.362659 \n",
            "loss at epoch 3540: 50.458900 \n",
            "loss at epoch 3560: 49.547508 \n",
            "loss at epoch 3580: 50.629318 \n",
            "loss at epoch 3600: 49.798706 \n",
            "loss at epoch 3620: 49.478886 \n",
            "loss at epoch 3640: 49.880520 \n",
            "loss at epoch 3660: 49.637638 \n",
            "loss at epoch 3680: 49.355881 \n",
            "loss at epoch 3700: 48.287434 \n",
            "loss at epoch 3720: 47.972626 \n",
            "loss at epoch 3740: 44.749893 \n",
            "loss at epoch 3760: 46.007359 \n",
            "loss at epoch 3780: 55.097370 \n",
            "loss at epoch 3800: 50.222412 \n",
            "loss at epoch 3820: 47.586643 \n",
            "loss at epoch 3840: 45.910553 \n",
            "loss at epoch 3860: 46.304520 \n",
            "loss at epoch 3880: 45.474831 \n",
            "loss at epoch 3900: 48.768867 \n",
            "loss at epoch 3920: 47.936283 \n",
            "loss at epoch 3940: 43.545609 \n",
            "loss at epoch 3960: 42.951103 \n",
            "loss at epoch 3980: 44.532043 \n",
            "loss at epoch 4000: 38.362915 \n",
            "loss at epoch 4020: 43.150681 \n",
            "loss at epoch 4040: 41.549110 \n",
            "loss at epoch 4060: 41.323723 \n",
            "loss at epoch 4080: 44.018772 \n",
            "loss at epoch 4100: 44.287514 \n",
            "loss at epoch 4120: 40.173080 \n",
            "loss at epoch 4140: 40.684780 \n",
            "loss at epoch 4160: 41.695065 \n",
            "loss at epoch 4180: 41.027931 \n",
            "loss at epoch 4200: 40.196102 \n",
            "loss at epoch 4220: 38.812550 \n",
            "loss at epoch 4240: 37.035583 \n",
            "loss at epoch 4260: 39.653625 \n",
            "loss at epoch 4280: 37.059158 \n",
            "loss at epoch 4300: 37.953568 \n",
            "loss at epoch 4320: 37.256115 \n",
            "loss at epoch 4340: 38.207314 \n",
            "loss at epoch 4360: 37.715981 \n",
            "loss at epoch 4380: 36.671597 \n",
            "loss at epoch 4400: 35.639145 \n",
            "loss at epoch 4420: 38.150284 \n",
            "loss at epoch 4440: 35.032242 \n",
            "loss at epoch 4460: 42.364437 \n",
            "loss at epoch 4480: 37.468483 \n",
            "loss at epoch 4500: 35.190292 \n",
            "loss at epoch 4520: 33.477264 \n",
            "loss at epoch 4540: 34.842812 \n",
            "loss at epoch 4560: 36.459461 \n",
            "loss at epoch 4580: 34.631840 \n",
            "loss at epoch 4600: 35.147881 \n",
            "loss at epoch 4620: 34.121910 \n",
            "loss at epoch 4640: 31.500977 \n",
            "loss at epoch 4660: 30.784786 \n",
            "loss at epoch 4680: 32.991623 \n",
            "loss at epoch 4700: 30.105738 \n",
            "loss at epoch 4720: 30.587072 \n",
            "loss at epoch 4740: 31.538034 \n",
            "loss at epoch 4760: 30.154175 \n",
            "loss at epoch 4780: 31.380459 \n",
            "loss at epoch 4800: 31.959919 \n",
            "loss at epoch 4820: 30.558973 \n",
            "loss at epoch 4840: 29.023813 \n",
            "loss at epoch 4860: 33.912567 \n",
            "loss at epoch 4880: 32.155846 \n",
            "loss at epoch 4900: 29.119732 \n",
            "loss at epoch 4920: 28.805393 \n",
            "loss at epoch 4940: 29.355179 \n",
            "loss at epoch 4960: 29.314842 \n",
            "loss at epoch 4980: 27.886496 \n",
            "loss at epoch 5000: 31.734499 \n",
            "loss at epoch 5020: 28.964695 \n",
            "loss at epoch 5040: 29.272514 \n",
            "loss at epoch 5060: 28.181581 \n",
            "loss at epoch 5080: 26.933666 \n",
            "loss at epoch 5100: 25.732288 \n",
            "loss at epoch 5120: 33.372658 \n",
            "loss at epoch 5140: 26.337120 \n",
            "loss at epoch 5160: 26.469280 \n",
            "loss at epoch 5180: 26.258446 \n",
            "loss at epoch 5200: 26.592987 \n",
            "loss at epoch 5220: 23.409983 \n",
            "loss at epoch 5240: 23.365202 \n",
            "loss at epoch 5260: 22.014297 \n",
            "loss at epoch 5280: 24.909252 \n",
            "loss at epoch 5300: 23.052658 \n",
            "loss at epoch 5320: 23.219494 \n",
            "loss at epoch 5340: 22.694405 \n",
            "loss at epoch 5360: 23.091152 \n",
            "loss at epoch 5380: 24.912289 \n",
            "loss at epoch 5400: 24.456617 \n",
            "loss at epoch 5420: 25.635735 \n",
            "loss at epoch 5440: 25.206379 \n",
            "loss at epoch 5460: 22.612846 \n",
            "loss at epoch 5480: 21.760893 \n",
            "loss at epoch 5500: 25.821846 \n",
            "loss at epoch 5520: 22.487379 \n",
            "loss at epoch 5540: 23.494839 \n",
            "loss at epoch 5560: 22.316509 \n",
            "loss at epoch 5580: 24.294022 \n",
            "loss at epoch 5600: 21.192867 \n",
            "loss at epoch 5620: 22.379753 \n",
            "loss at epoch 5640: 23.045582 \n",
            "loss at epoch 5660: 20.853022 \n",
            "loss at epoch 5680: 21.232082 \n",
            "loss at epoch 5700: 21.667770 \n",
            "loss at epoch 5720: 23.965576 \n",
            "loss at epoch 5740: 22.909739 \n",
            "loss at epoch 5760: 21.456520 \n",
            "loss at epoch 5780: 21.951660 \n",
            "loss at epoch 5800: 21.663767 \n",
            "loss at epoch 5820: 21.688068 \n",
            "loss at epoch 5840: 21.193624 \n",
            "loss at epoch 5860: 23.796507 \n",
            "loss at epoch 5880: 20.077070 \n",
            "loss at epoch 5900: 20.583214 \n",
            "loss at epoch 5920: 21.798946 \n",
            "loss at epoch 5940: 22.926010 \n",
            "loss at epoch 5960: 20.185532 \n",
            "loss at epoch 5980: 20.563086 \n",
            "loss at epoch 6000: 20.106941 \n",
            "loss at epoch 6020: 20.583645 \n",
            "loss at epoch 6040: 20.765020 \n",
            "loss at epoch 6060: 23.638149 \n",
            "loss at epoch 6080: 22.669346 \n",
            "loss at epoch 6100: 20.922897 \n",
            "loss at epoch 6120: 20.172993 \n",
            "loss at epoch 6140: 19.105186 \n",
            "loss at epoch 6160: 19.826075 \n",
            "loss at epoch 6180: 19.394316 \n",
            "loss at epoch 6200: 19.361897 \n",
            "loss at epoch 6220: 20.375647 \n",
            "loss at epoch 6240: 20.075880 \n",
            "loss at epoch 6260: 19.155369 \n",
            "loss at epoch 6280: 20.120354 \n",
            "loss at epoch 6300: 18.256577 \n",
            "loss at epoch 6320: 18.967045 \n",
            "loss at epoch 6340: 18.250263 \n",
            "loss at epoch 6360: 18.590271 \n",
            "loss at epoch 6380: 18.527401 \n",
            "loss at epoch 6400: 18.579796 \n",
            "loss at epoch 6420: 18.890799 \n",
            "loss at epoch 6440: 19.691277 \n",
            "loss at epoch 6460: 21.951126 \n",
            "loss at epoch 6480: 20.363054 \n",
            "loss at epoch 6500: 18.741011 \n",
            "loss at epoch 6520: 16.786652 \n",
            "loss at epoch 6540: 17.786053 \n",
            "loss at epoch 6560: 17.905031 \n",
            "loss at epoch 6580: 14.869390 \n",
            "loss at epoch 6600: 16.559298 \n",
            "loss at epoch 6620: 15.893003 \n",
            "loss at epoch 6640: 15.633821 \n",
            "loss at epoch 6660: 14.110884 \n",
            "loss at epoch 6680: 14.602953 \n",
            "loss at epoch 6700: 15.302954 \n",
            "loss at epoch 6720: 15.055418 \n",
            "loss at epoch 6740: 13.630376 \n",
            "loss at epoch 6760: 15.131378 \n",
            "loss at epoch 6780: 14.618641 \n",
            "loss at epoch 6800: 13.906977 \n",
            "loss at epoch 6820: 14.007959 \n",
            "loss at epoch 6840: 12.871262 \n",
            "loss at epoch 6860: 13.313421 \n",
            "loss at epoch 6880: 12.762182 \n",
            "loss at epoch 6900: 14.336312 \n",
            "loss at epoch 6920: 12.935310 \n",
            "loss at epoch 6940: 11.862453 \n",
            "loss at epoch 6960: 12.500716 \n",
            "loss at epoch 6980: 11.745564 \n",
            "loss at epoch 7000: 13.750839 \n",
            "loss at epoch 7020: 12.570947 \n",
            "loss at epoch 7040: 12.845509 \n",
            "loss at epoch 7060: 13.688384 \n",
            "loss at epoch 7080: 10.860653 \n",
            "loss at epoch 7100: 11.228447 \n",
            "loss at epoch 7120: 11.184422 \n",
            "loss at epoch 7140: 10.161350 \n",
            "loss at epoch 7160: 12.167170 \n",
            "loss at epoch 7180: 11.288269 \n",
            "loss at epoch 7200: 11.491825 \n",
            "loss at epoch 7220: 10.580740 \n",
            "loss at epoch 7240: 10.799587 \n",
            "loss at epoch 7260: 10.691339 \n",
            "loss at epoch 7280: 11.006316 \n",
            "loss at epoch 7300: 10.517508 \n",
            "loss at epoch 7320: 9.514044 \n",
            "loss at epoch 7340: 11.442216 \n",
            "loss at epoch 7360: 9.979968 \n",
            "loss at epoch 7380: 9.819513 \n",
            "loss at epoch 7400: 10.409817 \n",
            "loss at epoch 7420: 9.784664 \n",
            "loss at epoch 7440: 9.877968 \n",
            "loss at epoch 7460: 10.489624 \n",
            "loss at epoch 7480: 9.950486 \n",
            "loss at epoch 7500: 8.952473 \n",
            "loss at epoch 7520: 11.668646 \n",
            "loss at epoch 7540: 12.598497 \n",
            "loss at epoch 7560: 8.792894 \n",
            "loss at epoch 7580: 10.512509 \n",
            "loss at epoch 7600: 8.319909 \n",
            "loss at epoch 7620: 7.740750 \n",
            "loss at epoch 7640: 8.402195 \n",
            "loss at epoch 7660: 9.162642 \n",
            "loss at epoch 7680: 7.954400 \n",
            "loss at epoch 7700: 8.491294 \n",
            "loss at epoch 7720: 8.950550 \n",
            "loss at epoch 7740: 8.427894 \n",
            "loss at epoch 7760: 8.913001 \n",
            "loss at epoch 7780: 8.864367 \n",
            "loss at epoch 7800: 8.007524 \n",
            "loss at epoch 7820: 8.550642 \n",
            "loss at epoch 7840: 7.867985 \n",
            "loss at epoch 7860: 7.695445 \n",
            "loss at epoch 7880: 8.163553 \n",
            "loss at epoch 7900: 6.506561 \n",
            "loss at epoch 7920: 8.141035 \n",
            "loss at epoch 7940: 6.769200 \n",
            "loss at epoch 7960: 7.390133 \n",
            "loss at epoch 7980: 9.349390 \n",
            "loss at epoch 8000: 8.720241 \n",
            "loss at epoch 8020: 7.338781 \n",
            "loss at epoch 8040: 8.949648 \n",
            "loss at epoch 8060: 6.806561 \n",
            "loss at epoch 8080: 6.412302 \n",
            "loss at epoch 8100: 6.527478 \n",
            "loss at epoch 8120: 7.215721 \n",
            "loss at epoch 8140: 7.378494 \n",
            "loss at epoch 8160: 6.606865 \n",
            "loss at epoch 8180: 7.360937 \n",
            "loss at epoch 8200: 6.080554 \n",
            "loss at epoch 8220: 6.596822 \n",
            "loss at epoch 8240: 6.723104 \n",
            "loss at epoch 8260: 6.297829 \n",
            "loss at epoch 8280: 5.940645 \n",
            "loss at epoch 8300: 6.310846 \n",
            "loss at epoch 8320: 6.703902 \n",
            "loss at epoch 8340: 5.423183 \n",
            "loss at epoch 8360: 5.210093 \n",
            "loss at epoch 8380: 156.689941 \n",
            "loss at epoch 8400: 102.873154 \n",
            "loss at epoch 8420: 24.485565 \n",
            "loss at epoch 8440: 16.696085 \n",
            "loss at epoch 8460: 12.531797 \n",
            "loss at epoch 8480: 13.361312 \n",
            "loss at epoch 8500: 14.410349 \n",
            "loss at epoch 8520: 10.679004 \n",
            "loss at epoch 8540: 11.877025 \n",
            "loss at epoch 8560: 10.087782 \n",
            "loss at epoch 8580: 10.022973 \n",
            "loss at epoch 8600: 8.657879 \n",
            "loss at epoch 8620: 9.302226 \n",
            "loss at epoch 8640: 9.034236 \n",
            "loss at epoch 8660: 8.420903 \n",
            "loss at epoch 8680: 9.308321 \n",
            "loss at epoch 8700: 7.227685 \n",
            "loss at epoch 8720: 9.274775 \n",
            "loss at epoch 8740: 7.701538 \n",
            "loss at epoch 8760: 7.326069 \n",
            "loss at epoch 8780: 9.191883 \n",
            "loss at epoch 8800: 7.461206 \n",
            "loss at epoch 8820: 8.978113 \n",
            "loss at epoch 8840: 23.362309 \n",
            "loss at epoch 8860: 74.138123 \n",
            "loss at epoch 8880: 39.727879 \n",
            "loss at epoch 8900: 14.956074 \n",
            "loss at epoch 8920: 11.313484 \n",
            "loss at epoch 8940: 9.999447 \n",
            "loss at epoch 8960: 11.331861 \n",
            "loss at epoch 8980: 8.841601 \n",
            "loss at epoch 9000: 8.934893 \n",
            "loss at epoch 9020: 7.403472 \n",
            "loss at epoch 9040: 7.712287 \n",
            "loss at epoch 9060: 9.491935 \n",
            "loss at epoch 9080: 8.012133 \n",
            "loss at epoch 9100: 7.204409 \n",
            "loss at epoch 9120: 7.456407 \n",
            "loss at epoch 9140: 7.306903 \n",
            "loss at epoch 9160: 6.377840 \n",
            "loss at epoch 9180: 6.877083 \n",
            "loss at epoch 9200: 7.209264 \n",
            "loss at epoch 9220: 8.553434 \n",
            "loss at epoch 9240: 7.645325 \n",
            "loss at epoch 9260: 7.054768 \n",
            "loss at epoch 9280: 7.250114 \n",
            "loss at epoch 9300: 6.628740 \n",
            "loss at epoch 9320: 5.720454 \n",
            "loss at epoch 9340: 7.256668 \n",
            "loss at epoch 9360: 6.489686 \n",
            "loss at epoch 9380: 6.792740 \n",
            "loss at epoch 9400: 5.955749 \n",
            "loss at epoch 9420: 7.010851 \n",
            "loss at epoch 9440: 5.883858 \n",
            "loss at epoch 9460: 5.551664 \n",
            "loss at epoch 9480: 5.544808 \n",
            "loss at epoch 9500: 5.932787 \n",
            "loss at epoch 9520: 4.911875 \n",
            "loss at epoch 9540: 4.937109 \n",
            "loss at epoch 9560: 5.453817 \n",
            "loss at epoch 9580: 5.244151 \n",
            "loss at epoch 9600: 4.930185 \n",
            "loss at epoch 9620: 5.693649 \n",
            "loss at epoch 9640: 4.780282 \n",
            "loss at epoch 9660: 4.486277 \n",
            "loss at epoch 9680: 5.833337 \n",
            "loss at epoch 9700: 4.342354 \n",
            "loss at epoch 9720: 4.298437 \n",
            "loss at epoch 9740: 4.495499 \n",
            "loss at epoch 9760: 5.050876 \n",
            "loss at epoch 9780: 5.531907 \n",
            "loss at epoch 9800: 4.264667 \n",
            "loss at epoch 9820: 4.949579 \n",
            "loss at epoch 9840: 4.711174 \n",
            "loss at epoch 9860: 4.762730 \n",
            "loss at epoch 9880: 4.309331 \n",
            "loss at epoch 9900: 4.841000 \n",
            "loss at epoch 9920: 4.512698 \n",
            "loss at epoch 9940: 4.037834 \n",
            "loss at epoch 9960: 4.264663 \n",
            "loss at epoch 9980: 4.007053 \n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "\n",
        "#Image_wise_matrix = Image_wise_matrix.to(device)\n",
        "mlp = Net()\n",
        "best_model = copy.deepcopy(mlp)\n",
        "best_loss= 9999\n",
        "#criterion = MarcLoss(C, lambd)\n",
        "criterion = compute_loss_cosine\n",
        "#optimizer = optim.SGD(mlp.parameters(), lr=0.01, momentum=0.5)\n",
        "#optimizer = optim.SGD(mlp.parameters(), lr=0.001, momentum=0.5)\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train = X_train.to(device)\n",
        "X_train.requires_grad = False\n",
        "mlp.to(device)\n",
        "mlp.train()\n",
        "loss = 99999\n",
        "\n",
        "import random\n",
        "\n",
        "manualSeed = 10\n",
        "np.random.seed(manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed_all(manualSeed)\n",
        "torch.backends.cudnn.enabled = False \n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "for epoch in range(10000): \n",
        "    optimizer.zero_grad()\n",
        "    output = mlp(X_train.float()).to(device)\n",
        "    label= C.to(device)\n",
        "    loss = criterion(output,label)\n",
        "    if loss.item()<best_loss:\n",
        "        best_model = copy.deepcopy(mlp)\n",
        "        best_loss = loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 20 == 0:           \n",
        "      #mlp.eval()     \n",
        "      print(\"loss at epoch %s: %f \"% (epoch, loss.item()))\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f99d4dbd",
      "metadata": {
        "id": "f99d4dbd"
      },
      "outputs": [],
      "source": [
        "aa = np.array(df_known_averaged_and_Novel)\n",
        "aaa = aa.astype(float)\n",
        "aaa = torch.tensor(aaa)\n",
        "aaa_dist = 1-sim_matrix(aaa,aaa)\n",
        "np.savetxt(\"PerStack_distances.csv\", aaa_dist, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1d445d1",
      "metadata": {
        "id": "c1d445d1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "X_train_pseudonovel = Concatenated_Modalities_Informative_Patches\n",
        "\n",
        "X_train_novel= X_train_pseudonovel\n",
        "\n",
        "print(X_train_pseudonovel.shape)\n",
        "Genus_List_With_Pseudonovel = GenusList + [\"TOTARA\",]*40\n",
        "\n",
        "len(Genus_List_With_Pseudonovel)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_novel = X_train_novel.to(device)\n",
        "\n",
        "output = mlp(X_train.float())\n",
        "#output_novel = mlp(X_train_novel.reshape(X_train_novel.shape[0],1,X_train_novel.shape[1]).float())\n",
        "\n",
        "output_novel = mlp(X_train_novel.to(device))\n",
        "\n",
        "\n",
        "output_novel.shape\n",
        "Dataset_With_Known_And_Novel = torch.vstack((output, output_novel))\n",
        "Dataset_With_Known_And_Novel.shape\n",
        "\n",
        "# k + n scheme\n",
        "\n",
        "Species_Index = Y;\n",
        "NumericSpeciesIndex = pd.to_numeric(Species_Index);\n",
        "Indices_and_Features = np.column_stack((NumericSpeciesIndex, output.cpu().detach().numpy()));\n",
        "df = pd.DataFrame(data=Indices_and_Features)\n",
        "Per_Species_Transformed_Features = df.groupby([0,]).mean()\n",
        "Per_Species_Transformed_Features;\n",
        "\n",
        "NumericSpeciesIndexNovel  = [\"Pseudonovel\",]*40\n",
        "\n",
        "\n",
        "Indices_and_Features_Novel = np.column_stack((NumericSpeciesIndexNovel, output_novel.cpu().detach().numpy()));\n",
        "df_novel = pd.DataFrame(data= Indices_and_Features_Novel)\n",
        "df_novel.groupby(by=0,axis = 0)\n",
        "\n",
        "df_known_averaged_and_Novel = pd.concat([Per_Species_Transformed_Features,df_novel])\n",
        "df_known_averaged_and_Novel  = df_known_averaged_and_Novel.iloc[: , 1:]\n",
        "\n",
        "# Average linkage to obtain cladogram (ground-truth phylogenetic tree) from the original distance matrix. \n",
        "import matplotlib\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#df_known_averaged_and_Novel_norm = (df_known_averaged_and_Novel-df_known_averaged_and_Novel.min())/(df_known_averaged_and_Novel.max()-df_known_averaged_and_Novel.min())\n",
        "\n",
        "linked = linkage(df_known_averaged_and_Novel, \"average\", metric = \"cosine\")\n",
        "\n",
        "\n",
        "Genera = [\"$\\it{P. acutifolius}$\",\"$\\it{P. archboldii}$\",\"$\\it{P. brasiliensis}$\",\"$\\it{P. brassii}$\",\"$\\it{P. coriaceus}$\",\"$\\it{P. drouynianus}$\",\"$\\it{P. elatus}$\",\"$\\it{P. elongatus}$\",\"$\\it{P. glaucus}$\",\"$\\it{P. gnidiodes}$\",\"$\\it{P. lambertii}$\",\"$\\it{P. lawrencei}$\",\"$\\it{P. lucienii}$\",\"$\\it{P. macrophyllus}$\",\"$\\it{P. matudae}$\",\"$\\it{P. milanjianus}$\",\"$\\it{P. neriifolius}$\",\"$\\it{P. nivalis}$\",\"$\\it{P. novae-caledoniae}$\",\"$\\it{P. nubigenus}$\",\"$\\it{P. oleifolius}$\",\"$\\it{P. parlatorei}$\",\"$\\it{P. pilgeri}$\",\"$\\it{P. polystachyus}$\",\"$\\it{P. salignus}$\",\"$\\it{P. sellowii}$\",\"$\\it{P. spinulosus}$\",\"$\\it{P. sylvestris}$\",\"$\\it{P. totara}$\",\"$\\it{P. urbanii}$\"];\n",
        "\n",
        "Genera.remove(\"$\\it{P. totara}$\")\n",
        "\n",
        "Genera_with_novel = Genera + [\"TOTARA\",]*40\n",
        "\n",
        "labelList = Genera_with_novel\n",
        "plt.figure(figsize=(12,12), dpi = 300)\n",
        "matplotlib.rc('xtick', labelsize=8) \n",
        "matplotlib.rc('ytick', labelsize=8) \n",
        "dendrogram(linked,\n",
        "            orientation='left',\n",
        "            labels=labelList,\n",
        "            distance_sort='descending',\n",
        "            show_leaf_counts=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d5435e",
      "metadata": {
        "id": "01d5435e"
      },
      "outputs": [],
      "source": [
        "aa = np.array(df_known_averaged_and_Novel)\n",
        "aaa = aa.astype(float)\n",
        "aaa = torch.tensor(aaa)\n",
        "aaa_dist = 1-sim_matrix(aaa,aaa)\n",
        "np.savetxt(\"PerPatch_distances_corrected.csv\", aaa_dist, delimiter=\",\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}