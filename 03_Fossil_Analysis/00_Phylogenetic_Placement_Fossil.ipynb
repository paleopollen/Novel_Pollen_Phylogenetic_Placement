{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1OTow_GGH8pQR4MZFk7xkYuU0V7XMBkiE",
      "authorship_tag": "ABX9TyOuORwn5/d4cAydZiiW7wX3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madaime2/Novel_Pollen_Phylogenetic_Placement/blob/main/03_Fossil_Analysis/00_Phylogenetic_Placement_Fossil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision \n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "absolute_path = os.path.dirname(\"/content/drive/MyDrive/Podocarpus_Final/Podocarpus_Project/\")\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "_Q0O-7IWpfbX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Â Load Concatenated features (CNN-learned features across three modalities) for both the known and unknown specimens\n",
        "import pandas as pd \n",
        "df_Features_Known = pd.read_csv(absolute_path + \"/Concatenated_Features_Three_Modalities_Known.csv\", header = None)\n",
        "df_Features_Unknown = pd.read_csv(absolute_path + \"/Feature_vector_specimen_Panama_4.25.csv\", header = None)"
      ],
      "metadata": {
        "id": "MlPS6g9EUp_m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Pairwise (species-level) Distance Matrix \n",
        "\n",
        "# C represents the distance matrix depicting the \"ground-truth\" inter-class distances (i.e. the phylogenetic distances separating all *known* taxa)\n",
        "# These are the classes (taxa) known to the model. \n",
        "\n",
        "# Podocarpus cophenetic distance patrix (all taxa)\n",
        "\n",
        "C = np.array([\n",
        "    [0,99.79615,63.13772,99.79615,63.13772,99.79615,99.79615,63.13772,99.79615,30.05496,63.13772,30.05496,99.79615,99.79615,63.13772,63.13772,99.79615,30.05496,99.79615,63.13772,63.13772,63.13772,99.79615,99.79615,72.74426,63.13772,99.79615,99.79615,12.58331,63.13772],\n",
        "    [99.79615,0,99.79615,36.20694,99.79615,65.49675,49.15364,99.79615,65.49675,99.79615,99.79615,99.79615,49.15364,28.1985,99.79615,99.79615,42.4938,99.79615,49.15364,99.79615,99.79615,99.79615,28.1985,49.15364,99.79615,99.79615,65.49675,49.15364,99.79615,99.79615],\n",
        "    [63.13772,99.79615,0,99.79615,15.93869,99.79615,99.79615,52.67842,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,33.41821,52.67842,99.79615,63.13772,99.79615,58.99733,25.55484,58.99733,99.79615,99.79615,72.74426,15.93869,99.79615,99.79615,63.13772,25.55484],\n",
        "    [99.79615,36.20694,99.79615,0,99.79615,65.49675,49.15363,99.79615,65.49675,99.79615,99.79615,99.79615,49.15363,36.20694,99.79615,99.79615,42.4938,99.79615,49.15363,99.79615,99.79615,99.79615,36.20694,49.15363,99.79615,99.79615,65.49675,49.15363,99.79615,99.79615],\n",
        "    [63.13772,99.79615,15.93869,99.79615,0,99.79615,99.79615,52.67842,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,33.41821,52.67842,99.79615,63.13772,99.79615,58.99733,25.55484,58.99733,99.79615,99.79615,72.74426,12.69516,99.79615,99.79615,63.13772,25.55484],\n",
        "    [99.79615,65.49675,99.79615,65.49675,99.79615,0,65.49675,99.79615,32.59701,99.79615,99.79615,99.79615,65.49675,65.49675,99.79615,99.79615,65.49675,99.79615,65.49675,99.79615,99.79615,99.79615,65.49675,65.49675,99.79615,99.79615,32.59701,65.49675,99.79615,99.79615],\n",
        "    [99.79615,49.15364,99.79615,49.15363,99.79615,65.49675,0,99.79615,65.49675,99.79615,99.79615,99.79615,28.27449,49.15363,99.79615,99.79615,49.15364,99.79615,28.27449,99.79615,99.79615,99.79615,49.15363,10.13461,99.79615,99.79615,65.49675,28.27449,99.79615,99.79615],\n",
        "    [63.13772,99.79615,52.67842,99.79615,52.67842,99.79615,99.79615,0,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,52.67842,12.04249,99.79615,63.13772,99.79615,58.99733,52.67842,58.99733,99.79615,99.79615,72.74426,52.67842,99.79615,99.79615,63.13772,52.67842],\n",
        "    [99.79615,65.49675,99.79615,65.49675,99.79615,32.59701,65.49675,99.79615,0,99.79615,99.79615,99.79615,65.49675,65.49675,99.79615,99.79615,65.49675,99.79615,65.49675,99.79615,99.79615,99.79615,65.49675,65.49675,99.79615,99.79615,2.84746,65.49675,99.79615,99.79615],\n",
        "    [30.05496,99.79615,63.13772,99.79615,63.13772,99.79615,99.79615,63.13772,99.79615,0,63.13772,7.36035,99.79615,99.79615,63.13772,63.13772,99.79615,14.47632,99.79615,63.13772,63.13772,63.13772,99.79615,99.79615,72.74426,63.13772,99.79615,99.79615,30.05496,63.13772],\n",
        "    [63.13772,99.79615,58.99733,99.79615,58.99733,99.79615,99.79615,58.99733,99.79615,63.13772,0,63.13772,99.79615,99.79615,58.99733,58.99733,99.79615,63.13772,99.79615,51.13068,58.99733,23.76483,99.79615,99.79615,72.74426,58.99733,99.79615,99.79615,63.13772,58.99733],\n",
        "    [30.05496,99.79615,63.13772,99.79615,63.13772,99.79615,99.79615,63.13772,99.79615,7.36035,63.13772,0,99.79615,99.79615,63.13772,63.13772,99.79615,14.47632,99.79615,63.13772,63.13772,63.13772,99.79615,99.79615,72.74426,63.13772,99.79615,99.79615,30.05496,63.13772],\n",
        "    [99.79615,49.15364,99.79615,49.15363,99.79615,65.49675,28.27449,99.79615,65.49675,99.79615,99.79615,99.79615,0,49.15363,99.79615,99.79615,49.15364,99.79615,20.10967,99.79615,99.79615,99.79615,49.15363,28.27449,99.79615,99.79615,65.49675,7.82913,99.79615,99.79615],\n",
        "    [99.79615,28.1985,99.79615,36.20694,99.79615,65.49675,49.15363,99.79615,65.49675,99.79615,99.79615,99.79615,49.15363,0,99.79615,99.79615,42.4938,99.79615,49.15363,99.79615,99.79615,99.79615,5.34205,49.15363,99.79615,99.79615,65.49675,49.15363,99.79615,99.79615],\n",
        "    [63.13772,99.79615,33.41821,99.79615,33.41821,99.79615,99.79615,52.67842,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,0,52.67842,99.79615,63.13772,99.79615,58.99733,33.41822,58.99733,99.79615,99.79615,72.74426,33.41821,99.79615,99.79615,63.13772,33.41821],\n",
        "    [63.13772,99.79615,52.67842,99.79615,52.67842,99.79615,99.79615,12.04249,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,52.67842,0,99.79615,63.13772,99.79615,58.99733,52.67842,58.99733,99.79615,99.79615,72.74426,52.67842,99.79615,99.79615,63.13772,52.67842],\n",
        "    [99.79615,42.4938,99.79615,42.4938,99.79615,65.49675,49.15364,99.79615,65.49675,99.79615,99.79615,99.79615,49.15364,42.4938,99.79615,99.79615,0,99.79615,49.15364,99.79615,99.79615,99.79615,42.4938,49.15364,99.79615,99.79615,65.49675,49.15364,99.79615,99.79615],\n",
        "    [30.05496,99.79615,63.13772,99.79615,63.13772,99.79615,99.79615,63.13772,99.79615,14.47632,63.13772,14.47632,99.79615,99.79615,63.13772,63.13772,99.79615,0,99.79615,63.13772,63.13772,63.13772,99.79615,99.79615,72.74426,63.13772,99.79615,99.79615,30.05496,63.13772],\n",
        "    [99.79615,49.15364,99.79615,49.15363,99.79615,65.49675,28.27449,99.79615,65.49675,99.79615,99.79615,99.79615,20.10967,49.15363,99.79615,99.79615,49.15364,99.79615,0,99.79615,99.79615,99.79615,49.15363,28.27449,99.79615,99.79615,65.49675,20.10967,99.79615,99.79615],\n",
        "    [63.13772,99.79615,58.99733,99.79615,58.99733,99.79615,99.79615,58.99733,99.79615,63.13772,51.13068,63.13772,99.79615,99.79615,58.99733,58.99733,99.79615,63.13772,99.79615,0,58.99733,51.13068,99.79615,99.79615,72.74426,58.99733,99.79615,99.79615,63.13772,58.99733],\n",
        "    [63.13772,99.79615,25.55484,99.79615,25.55484,99.79615,99.79615,52.67842,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,33.41822,52.67842,99.79615,63.13772,99.79615,58.99733,0,58.99733,99.79615,99.79615,72.74426,25.55484,99.79615,99.79615,63.13772,22.27456],\n",
        "    [63.13772,99.79615,58.99733,99.79615,58.99733,99.79615,99.79615,58.99733,99.79615,63.13772,23.76483,63.13772,99.79615,99.79615,58.99733,58.99733,99.79615,63.13772,99.79615,51.13068,58.99733,0,99.79615,99.79615,72.74426,58.99733,99.79615,99.79615,63.13772,58.99733],\n",
        "    [99.79615,28.1985,99.79615,36.20694,99.79615,65.49675,49.15363,99.79615,65.49675,99.79615,99.79615,99.79615,49.15363,5.34205,99.79615,99.79615,42.4938,99.79615,49.15363,99.79615,99.79615,99.79615,0,49.15363,99.79615,99.79615,65.49675,49.15363,99.79615,99.79615],\n",
        "    [99.79615,49.15364,99.79615,49.15363,99.79615,65.49675,10.13461,99.79615,65.49675,99.79615,99.79615,99.79615,28.27449,49.15363,99.79615,99.79615,49.15364,99.79615,28.27449,99.79615,99.79615,99.79615,49.15363,0,99.79615,99.79615,65.49675,28.27449,99.79615,99.79615],\n",
        "    [72.74426,99.79615,72.74426,99.79615,72.74426,99.79615,99.79615,72.74426,99.79615,72.74426,72.74426,72.74426,99.79615,99.79615,72.74426,72.74426,99.79615,72.74426,99.79615,72.74426,72.74426,72.74426,99.79615,99.79615,0,72.74426,99.79615,99.79615,72.74426,72.74426],\n",
        "    [63.13772,99.79615,15.93869,99.79615,12.69516,99.79615,99.79615,52.67842,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,33.41821,52.67842,99.79615,63.13772,99.79615,58.99733,25.55484,58.99733,99.79615,99.79615,72.74426,0,99.79615,99.79615,63.13772,25.55484],\n",
        "    [99.79615,65.49675,99.79615,65.49675,99.79615,32.59701,65.49675,99.79615,2.84746,99.79615,99.79615,99.79615,65.49675,65.49675,99.79615,99.79615,65.49675,99.79615,65.49675,99.79615,99.79615,99.79615,65.49675,65.49675,99.79615,99.79615,0,65.49675,99.79615,99.79615],\n",
        "    [99.79615,49.15364,99.79615,49.15363,99.79615,65.49675,28.27449,99.79615,65.49675,99.79615,99.79615,99.79615,7.82913,49.15363,99.79615,99.79615,49.15364,99.79615,20.10967,99.79615,99.79615,99.79615,49.15363,28.27449,99.79615,99.79615,65.49675,0,99.79615,99.79615],\n",
        "    [12.58331,99.79615,63.13772,99.79615,63.13772,99.79615,99.79615,63.13772,99.79615,30.05496,63.13772,30.05496,99.79615,99.79615,63.13772,63.13772,99.79615,30.05496,99.79615,63.13772,63.13772,63.13772,99.79615,99.79615,72.74426,63.13772,99.79615,99.79615,0,63.13772],\n",
        "    [63.13772,99.79615,25.55484,99.79615,25.55484,99.79615,99.79615,52.67842,99.79615,63.13772,58.99733,63.13772,99.79615,99.79615,33.41821,52.67842,99.79615,63.13772,99.79615,58.99733,22.27456,58.99733,99.79615,99.79615,72.74426,25.55484,99.79615,99.79615,63.13772,0]\n",
        "    ])\n",
        "\n",
        "# Normalize (0-1) distances \n",
        "C = C/C.max()\n",
        "C = torch.from_numpy(C)\n",
        "print(C.shape)\n",
        "\n",
        "# Load all species names/indices \n",
        "Y = [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1, 1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  3, 3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4, 4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,  6, 6,  6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8, 8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29]\n",
        "Genera = [\"$\\it{P. acutifolius}$\",\"$\\it{P. archboldii}$\",\"$\\it{P. brasiliensis}$\",\"$\\it{P. brassii}$\",\"$\\it{P. coriaceus}$\",\"$\\it{P. drouynianus}$\",\"$\\it{P. elatus}$\",\"$\\it{P. elongatus}$\",\"$\\it{P. glaucus}$\",\"$\\it{P. gnidiodes}$\",\"$\\it{P. lambertii}$\",\"$\\it{P. lawrencei}$\",\"$\\it{P. lucienii}$\",\"$\\it{P. macrophyllus}$\",\"$\\it{P. matudae}$\",\"$\\it{P. milanjianus}$\",\"$\\it{P. neriifolius}$\",\"$\\it{P. nivalis}$\",\"$\\it{P. novae-caledoniae}$\",\"$\\it{P. nubigenus}$\", \"$\\it{P. oleifolius}$\",\"$\\it{P. parlatorei}$\",\"$\\it{P. pilgeri}$\",\"$\\it{P. polystachyus}$\",\"$\\it{P. salignus}$\",\"$\\it{P. sellowii}$\",\"$\\it{P. spinulosus}$\",\"$\\it{P. sylvestris}$\",\"$\\it{P. totara}$\",\"$\\it{P. urbanii}$\"];\n",
        "GenusList = [] \n",
        "for i in range(len(Y)):\n",
        " GenusNumber = Y[i]\n",
        " GenusName = Genera[GenusNumber]\n",
        " GenusList.append(GenusName)\n",
        "\n",
        "# Load features (i.e. input to the multilayer perceptron) for the entire detaset\n",
        "import torch\n",
        "import pandas as pd\n",
        "X_train = torch.tensor(df_Features_Known.values)\n",
        "print(X_train.shape)\n",
        "Y_tens = torch.tensor(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mbQKp52VnQ4",
        "outputId": "0f7def81-a689-4a28-e9ef-85ccde344e2b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 30])\n",
            "torch.Size([309, 6144])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3EHfLe4AhCSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b0ffe6-e098-4d4c-a305-8acd2b4fc210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([309, 309])\n"
          ]
        }
      ],
      "source": [
        "# Compute image-wise distance matrix from class-wise distance matrix (i.e. convert the species-level pairwise distance matrix to a specimen-level pairwise distance matrix)\n",
        "\n",
        "# We now have a distance matrix depicting evolutionary distances separating each and every *specimen* (not only species)\n",
        "\n",
        "repeated_rows = []\n",
        "\n",
        "for i in range(len(C)):\n",
        "  a = torch.Tensor(C[i].float())\n",
        "  a_repeat = a.repeat(torch.bincount(Y_tens).numpy()[i],1)\n",
        "  repeated_rows.append(a_repeat)\n",
        "  repeated_rows\n",
        "  repeated_rows_cat = torch.cat(repeated_rows,0)\n",
        "  repeated_rows_cat_transpose = torch.transpose(repeated_rows_cat,0,1)\n",
        "  repeated_rows_transpose = []\n",
        "  \n",
        "  for j in range(len(repeated_rows_cat_transpose)):\n",
        "    b = torch.Tensor(repeated_rows_cat_transpose[j])\n",
        "    b_repeat = b.repeat(torch.bincount(Y_tens).numpy()[j],1)\n",
        "    repeated_rows_transpose.append(b_repeat)\n",
        "    Image_wise_matrix = torch.cat(repeated_rows_transpose)\n",
        "\n",
        "print(Image_wise_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "87583J_dhCUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c24ee8-b47b-43a2-afb8-a172d614d571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(X_train)\n",
        "y = Image_wise_matrix\n",
        "#labels = np.array(y)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "labels = y;\n",
        "labels.to(device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Lxh2PN-3hCXF"
      },
      "outputs": [],
      "source": [
        "# Euclidean Distance-Based Loss function \n",
        "\n",
        "def compute_loss(output, label = Image_wise_matrix):\n",
        "    distance_mat = torch.cdist(output, output, p=2)\n",
        "    distance_mat = distance_mat/distance_mat.max()\n",
        "    return torch.sum(torch.pow(Image_wise_matrix-distance_mat, 2))\n",
        "    #return torch.sum(torch.abs(Image_wise_matrix - distance_mat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wqsgd4Txkv5H"
      },
      "outputs": [],
      "source": [
        "# Cosine Distance-Based Loss Function \n",
        "\n",
        "def sim_matrix(a, b, eps=1e-8):\n",
        "    \"\"\"\n",
        "    added eps for numerical stability\n",
        "    \"\"\"\n",
        "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
        "    a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
        "    b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
        "    sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
        "    #sim_mt = a_norm@b_norm.T \n",
        "    return sim_mt\n",
        "\n",
        "def compute_loss_cosine(output, label = Image_wise_matrix):\n",
        "    distance_mat = 1 - sim_matrix(output, output, eps = 1e-8)\n",
        "    #distance_mat = distance_mat/distance_mat.max()\n",
        "    return torch.sum(torch.pow(Image_wise_matrix-distance_mat, 2))\n",
        "    #return torch.sum(torch.abs(Image_wise_matrix - distance_mat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "85L9-J0Ckv7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd363cf-8fb5-4b95-cc89-901d0a7559ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([309, 6144])\n"
          ]
        }
      ],
      "source": [
        "# Get training data \n",
        "\n",
        "train_data = []\n",
        "for i in range(len(x)):\n",
        "   train_data.append([x[i], labels[i]])\n",
        "   trainloader = torch.utils.data.DataLoader(train_data, shuffle=False, batch_size=len(y))\n",
        "   i1, l1 = next(iter(trainloader))\n",
        "print(i1.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "t-lQZXcekv9a"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# define the NN architecture\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(6144 , 1024)\n",
        "\n",
        "        # Add \n",
        "\n",
        "        #self.a = torch.nn.parameter.Parameter(torch.tensor([1.], requires_grad = False))\n",
        "        #self.b = torch.nn.parameter.Parameter(torch.tensor([1.], requires_grad = False))\n",
        "        #self.c = torch.nn.parameter.Parameter(torch.tensor([1.], requires_grad = False))\n",
        "\n",
        "        self.a = nn.Parameter(torch.ones(1))\n",
        "        self.b = nn.Parameter(torch.ones(1))\n",
        "        self.c = nn.Parameter(torch.ones(1))\n",
        "\n",
        "        #torch.nn.Parameter(torch.tensor([1.], requires_grad = False))\n",
        "\n",
        "        # End \n",
        "\n",
        "        #self.dropout1 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(1024 , 512)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(512 , 256)\n",
        "        self.dropout3 = nn.Dropout(0.5)\n",
        "        self.fc4 = nn.Linear(256 , 256)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # flatten image input\n",
        "        x = x.view(-1, 6144)\n",
        "\n",
        "        aa = F.sigmoid(self.a)\n",
        "        bb = F.sigmoid(self.b)\n",
        "        cc = F.sigmoid(self.c)\n",
        "\n",
        "        aa,bb,cc = aa/(aa+bb+cc),bb/(aa+bb+cc),cc/(aa+bb+cc)\n",
        "\n",
        "        #print(\"test place 1\")\n",
        "        # Adding a \"weight\" to each of the three modalities \n",
        "        # Allows us to look at their individual contributions during training. \n",
        "        \n",
        "        x1 = aa*x[:, 0:2048]\n",
        "        x2 = bb*x[:, 2048:4096]\n",
        "        x3 = cc*x[:, 4096:6144]\n",
        "        xx = torch.cat([x1,x2,x3],1)\n",
        "        #print(\"test place 2\")\n",
        "        x = F.relu(self.fc1(xx))  \n",
        "        #x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.dropout3(x)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "W05AXeVQlABF"
      },
      "outputs": [],
      "source": [
        "labels.to(device);\n",
        "X_train = X_train.to(device);\n",
        "x = x.to(device);\n",
        "y = y.to(device);\n",
        "Image_wise_matrix = Image_wise_matrix.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "\n",
        "#Image_wise_matrix = Image_wise_matrix.to(device)\n",
        "mlp = Net()\n",
        "best_model = copy.deepcopy(mlp)\n",
        "best_loss= 9999\n",
        "criterion = compute_loss_cosine\n",
        "optimizer = optim.Adam(mlp.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train = X_train.to(device)\n",
        "X_train.requires_grad = False\n",
        "mlp.to(device)\n",
        "mlp.train()\n",
        "loss = 99999\n",
        "\n",
        "import random\n",
        "\n",
        "#manualSeed = 7\n",
        "#np.random.seed(manualSeed)\n",
        "#random.seed(manualSeed)\n",
        "#torch.manual_seed(manualSeed)\n",
        "#torch.cuda.manual_seed(manualSeed)\n",
        "#torch.cuda.manual_seed_all(manualSeed)\n",
        "#torch.backends.cudnn.enabled = False \n",
        "#torch.backends.cudnn.benchmark = False\n",
        "#torch.backends.cudnn.deterministic = True\n",
        "\n",
        "for epoch in range(10000): \n",
        "    optimizer.zero_grad()\n",
        "    output = mlp(X_train.float()).to(device)\n",
        "    label= C.to(device)\n",
        "    loss = criterion(output,label)\n",
        "    if loss.item()<best_loss:\n",
        "        best_model = copy.deepcopy(mlp)\n",
        "        best_loss = loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 20 == 0:           \n",
        "      #mlp.eval()     \n",
        "      print(\"loss at epoch %s: %f \"% (epoch, loss.item()))\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgOINmgKaCos",
        "outputId": "1424a320-8e5f-4f10-ca17-6a97f20a9e42"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss at epoch 0: 38180.777344 \n",
            "loss at epoch 20: 5140.976074 \n",
            "loss at epoch 40: 1261.216309 \n",
            "loss at epoch 60: 395.428162 \n",
            "loss at epoch 80: 193.123566 \n",
            "loss at epoch 100: 127.842499 \n",
            "loss at epoch 120: 87.533272 \n",
            "loss at epoch 140: 68.787506 \n",
            "loss at epoch 160: 42.678997 \n",
            "loss at epoch 180: 56.346996 \n",
            "loss at epoch 200: 26.237549 \n",
            "loss at epoch 220: 50.721279 \n",
            "loss at epoch 240: 30.928034 \n",
            "loss at epoch 260: 15.199020 \n",
            "loss at epoch 280: 12.171724 \n",
            "loss at epoch 300: 10.073151 \n",
            "loss at epoch 320: 27.031792 \n",
            "loss at epoch 340: 11.320205 \n",
            "loss at epoch 360: 7.323428 \n",
            "loss at epoch 380: 6.089292 \n",
            "loss at epoch 400: 30.017723 \n",
            "loss at epoch 420: 7.555036 \n",
            "loss at epoch 440: 5.108664 \n",
            "loss at epoch 460: 6.881592 \n",
            "loss at epoch 480: 7.007670 \n",
            "loss at epoch 500: 3.786802 \n",
            "loss at epoch 520: 3.319150 \n",
            "loss at epoch 540: 11.320551 \n",
            "loss at epoch 560: 4.316858 \n",
            "loss at epoch 580: 2.643751 \n",
            "loss at epoch 600: 2.773823 \n",
            "loss at epoch 620: 7.694210 \n",
            "loss at epoch 640: 2.524867 \n",
            "loss at epoch 660: 2.274615 \n",
            "loss at epoch 680: 4.928153 \n",
            "loss at epoch 700: 4.590261 \n",
            "loss at epoch 720: 3.289837 \n",
            "loss at epoch 740: 1.916792 \n",
            "loss at epoch 760: 3.229402 \n",
            "loss at epoch 780: 3.060050 \n",
            "loss at epoch 800: 1.781536 \n",
            "loss at epoch 820: 1.465062 \n",
            "loss at epoch 840: 12.577831 \n",
            "loss at epoch 860: 7.821076 \n",
            "loss at epoch 880: 5.320052 \n",
            "loss at epoch 900: 1.484731 \n",
            "loss at epoch 920: 1.206725 \n",
            "loss at epoch 940: 1.034996 \n",
            "loss at epoch 960: 41.241467 \n",
            "loss at epoch 980: 2.483841 \n",
            "loss at epoch 1000: 1.616262 \n",
            "loss at epoch 1020: 0.975617 \n",
            "loss at epoch 1040: 0.866925 \n",
            "loss at epoch 1060: 0.800415 \n",
            "loss at epoch 1080: 0.745094 \n",
            "loss at epoch 1100: 1.941610 \n",
            "loss at epoch 1120: 5.330225 \n",
            "loss at epoch 1140: 1.266145 \n",
            "loss at epoch 1160: 0.700484 \n",
            "loss at epoch 1180: 2.460168 \n",
            "loss at epoch 1200: 1.583668 \n",
            "loss at epoch 1220: 0.847115 \n",
            "loss at epoch 1240: 1.727370 \n",
            "loss at epoch 1260: 1.989397 \n",
            "loss at epoch 1280: 1.676556 \n",
            "loss at epoch 1300: 0.565693 \n",
            "loss at epoch 1320: 3.161345 \n",
            "loss at epoch 1340: 0.679946 \n",
            "loss at epoch 1360: 0.882299 \n",
            "loss at epoch 1380: 1.152813 \n",
            "loss at epoch 1400: 8.857693 \n",
            "loss at epoch 1420: 0.852508 \n",
            "loss at epoch 1440: 0.477959 \n",
            "loss at epoch 1460: 0.429689 \n",
            "loss at epoch 1480: 2.514895 \n",
            "loss at epoch 1500: 0.738188 \n",
            "loss at epoch 1520: 1.849835 \n",
            "loss at epoch 1540: 2.469574 \n",
            "loss at epoch 1560: 0.423864 \n",
            "loss at epoch 1580: 1.711389 \n",
            "loss at epoch 1600: 1.236239 \n",
            "loss at epoch 1620: 1.119088 \n",
            "loss at epoch 1640: 5.686533 \n",
            "loss at epoch 1660: 0.499251 \n",
            "loss at epoch 1680: 0.536609 \n",
            "loss at epoch 1700: 0.395561 \n",
            "loss at epoch 1720: 1.492147 \n",
            "loss at epoch 1740: 1.600240 \n",
            "loss at epoch 1760: 2.261374 \n",
            "loss at epoch 1780: 0.875739 \n",
            "loss at epoch 1800: 3580.957764 \n",
            "loss at epoch 1820: 576.094788 \n",
            "loss at epoch 1840: 255.867371 \n",
            "loss at epoch 1860: 160.613235 \n",
            "loss at epoch 1880: 112.576347 \n",
            "loss at epoch 1900: 88.661659 \n",
            "loss at epoch 1920: 114.261383 \n",
            "loss at epoch 1940: 75.386765 \n",
            "loss at epoch 1960: 37.887173 \n",
            "loss at epoch 1980: 75.404411 \n",
            "loss at epoch 2000: 30.800888 \n",
            "loss at epoch 2020: 18.601814 \n",
            "loss at epoch 2040: 14.826741 \n",
            "loss at epoch 2060: 12.546274 \n",
            "loss at epoch 2080: 10.814404 \n",
            "loss at epoch 2100: 9.481524 \n",
            "loss at epoch 2120: 11.555890 \n",
            "loss at epoch 2140: 8.287651 \n",
            "loss at epoch 2160: 7.099770 \n",
            "loss at epoch 2180: 6.324449 \n",
            "loss at epoch 2200: 9.835743 \n",
            "loss at epoch 2220: 5.882989 \n",
            "loss at epoch 2240: 5.020297 \n",
            "loss at epoch 2260: 7.219657 \n",
            "loss at epoch 2280: 5.102833 \n",
            "loss at epoch 2300: 5.596273 \n",
            "loss at epoch 2320: 5.230044 \n",
            "loss at epoch 2340: 4.835587 \n",
            "loss at epoch 2360: 4.252859 \n",
            "loss at epoch 2380: 4.429557 \n",
            "loss at epoch 2400: 2.947700 \n",
            "loss at epoch 2420: 13.205661 \n",
            "loss at epoch 2440: 3.310757 \n",
            "loss at epoch 2460: 2.543346 \n",
            "loss at epoch 2480: 2.409175 \n",
            "loss at epoch 2500: 3.988897 \n",
            "loss at epoch 2520: 2.822639 \n",
            "loss at epoch 2540: 2.147369 \n",
            "loss at epoch 2560: 5.188942 \n",
            "loss at epoch 2580: 2.534870 \n",
            "loss at epoch 2600: 3.222929 \n",
            "loss at epoch 2620: 2.093169 \n",
            "loss at epoch 2640: 1.761040 \n",
            "loss at epoch 2660: 1.573002 \n",
            "loss at epoch 2680: 13.976679 \n",
            "loss at epoch 2700: 2.118682 \n",
            "loss at epoch 2720: 1.515219 \n",
            "loss at epoch 2740: 1.352554 \n",
            "loss at epoch 2760: 5.071860 \n",
            "loss at epoch 2780: 1.684480 \n",
            "loss at epoch 2800: 1.326318 \n",
            "loss at epoch 2820: 1.534679 \n",
            "loss at epoch 2840: 2.533330 \n",
            "loss at epoch 2860: 1.484892 \n",
            "loss at epoch 2880: 1.080326 \n",
            "loss at epoch 2900: 2.580898 \n",
            "loss at epoch 2920: 1.110049 \n",
            "loss at epoch 2940: 1.040004 \n",
            "loss at epoch 2960: 11.464871 \n",
            "loss at epoch 2980: 2.006609 \n",
            "loss at epoch 3000: 1.020309 \n",
            "loss at epoch 3020: 0.923703 \n",
            "loss at epoch 3040: 1.500952 \n",
            "loss at epoch 3060: 1.102643 \n",
            "loss at epoch 3080: 1.707633 \n",
            "loss at epoch 3100: 1.459968 \n",
            "loss at epoch 3120: 0.904788 \n",
            "loss at epoch 3140: 0.716898 \n",
            "loss at epoch 3160: 1.447131 \n",
            "loss at epoch 3180: 2.310874 \n",
            "loss at epoch 3200: 0.684926 \n",
            "loss at epoch 3220: 1.288361 \n",
            "loss at epoch 3240: 4.596215 \n",
            "loss at epoch 3260: 1.179222 \n",
            "loss at epoch 3280: 0.641970 \n",
            "loss at epoch 3300: 0.561828 \n",
            "loss at epoch 3320: 3.768359 \n",
            "loss at epoch 3340: 1.285895 \n",
            "loss at epoch 3360: 0.625992 \n",
            "loss at epoch 3380: 0.557499 \n",
            "loss at epoch 3400: 4.582009 \n",
            "loss at epoch 3420: 1.662453 \n",
            "loss at epoch 3440: 0.614906 \n",
            "loss at epoch 3460: 0.481927 \n",
            "loss at epoch 3480: 2.441747 \n",
            "loss at epoch 3500: 1.429935 \n",
            "loss at epoch 3520: 0.849273 \n",
            "loss at epoch 3540: 0.665081 \n",
            "loss at epoch 3560: 0.849111 \n",
            "loss at epoch 3580: 7.399529 \n",
            "loss at epoch 3600: 1.587155 \n",
            "loss at epoch 3620: 0.446462 \n",
            "loss at epoch 3640: 0.389319 \n",
            "loss at epoch 3660: 0.363129 \n",
            "loss at epoch 3680: 1.343450 \n",
            "loss at epoch 3700: 0.405061 \n",
            "loss at epoch 3720: 0.323626 \n",
            "loss at epoch 3740: 1.061659 \n",
            "loss at epoch 3760: 1.528120 \n",
            "loss at epoch 3780: 0.315507 \n",
            "loss at epoch 3800: 0.345167 \n",
            "loss at epoch 3820: 2.923713 \n",
            "loss at epoch 3840: 0.510919 \n",
            "loss at epoch 3860: 1.049906 \n",
            "loss at epoch 3880: 0.675690 \n",
            "loss at epoch 3900: 7.048460 \n",
            "loss at epoch 3920: 0.672719 \n",
            "loss at epoch 3940: 0.382936 \n",
            "loss at epoch 3960: 0.340419 \n",
            "loss at epoch 3980: 0.384473 \n",
            "loss at epoch 4000: 1.538601 \n",
            "loss at epoch 4020: 0.549772 \n",
            "loss at epoch 4040: 3.979468 \n",
            "loss at epoch 4060: 0.819657 \n",
            "loss at epoch 4080: 0.486549 \n",
            "loss at epoch 4100: 0.495439 \n",
            "loss at epoch 4120: 0.258216 \n",
            "loss at epoch 4140: 12.479336 \n",
            "loss at epoch 4160: 0.553695 \n",
            "loss at epoch 4180: 0.313324 \n",
            "loss at epoch 4200: 0.331876 \n",
            "loss at epoch 4220: 0.215849 \n",
            "loss at epoch 4240: 0.592414 \n",
            "loss at epoch 4260: 0.718432 \n",
            "loss at epoch 4280: 0.747035 \n",
            "loss at epoch 4300: 1.400426 \n",
            "loss at epoch 4320: 0.553737 \n",
            "loss at epoch 4340: 0.298264 \n",
            "loss at epoch 4360: 0.601988 \n",
            "loss at epoch 4380: 0.298114 \n",
            "loss at epoch 4400: 5.990418 \n",
            "loss at epoch 4420: 0.719154 \n",
            "loss at epoch 4440: 0.277813 \n",
            "loss at epoch 4460: 0.196814 \n",
            "loss at epoch 4480: 0.757719 \n",
            "loss at epoch 4500: 1.018718 \n",
            "loss at epoch 4520: 0.296019 \n",
            "loss at epoch 4540: 0.217642 \n",
            "loss at epoch 4560: 2.814029 \n",
            "loss at epoch 4580: 0.225416 \n",
            "loss at epoch 4600: 0.260063 \n",
            "loss at epoch 4620: 4.121357 \n",
            "loss at epoch 4640: 0.553001 \n",
            "loss at epoch 4660: 0.993402 \n",
            "loss at epoch 4680: 1.791078 \n",
            "loss at epoch 4700: 0.399331 \n",
            "loss at epoch 4720: 0.190253 \n",
            "loss at epoch 4740: 1.001078 \n",
            "loss at epoch 4760: 0.548125 \n",
            "loss at epoch 4780: 0.752294 \n",
            "loss at epoch 4800: 0.278320 \n",
            "loss at epoch 4820: 1.027396 \n",
            "loss at epoch 4840: 2.735989 \n",
            "loss at epoch 4860: 0.739965 \n",
            "loss at epoch 4880: 0.205406 \n",
            "loss at epoch 4900: 0.132352 \n",
            "loss at epoch 4920: 0.274095 \n",
            "loss at epoch 4940: 6.914559 \n",
            "loss at epoch 4960: 0.799441 \n",
            "loss at epoch 4980: 0.222754 \n",
            "loss at epoch 5000: 0.123278 \n",
            "loss at epoch 5020: 0.110611 \n",
            "loss at epoch 5040: 1.942452 \n",
            "loss at epoch 5060: 0.416292 \n",
            "loss at epoch 5080: 0.094833 \n",
            "loss at epoch 5100: 0.258122 \n",
            "loss at epoch 5120: 0.371552 \n",
            "loss at epoch 5140: 0.118307 \n",
            "loss at epoch 5160: 0.229186 \n",
            "loss at epoch 5180: 0.200638 \n",
            "loss at epoch 5200: 0.921938 \n",
            "loss at epoch 5220: 0.143537 \n",
            "loss at epoch 5240: 0.407393 \n",
            "loss at epoch 5260: 0.296218 \n",
            "loss at epoch 5280: 1.083214 \n",
            "loss at epoch 5300: 0.664533 \n",
            "loss at epoch 5320: 4.847559 \n",
            "loss at epoch 5340: 0.839390 \n",
            "loss at epoch 5360: 0.292977 \n",
            "loss at epoch 5380: 0.121882 \n",
            "loss at epoch 5400: 0.178691 \n",
            "loss at epoch 5420: 0.390903 \n",
            "loss at epoch 5440: 0.304886 \n",
            "loss at epoch 5460: 1.885645 \n",
            "loss at epoch 5480: 0.910191 \n",
            "loss at epoch 5500: 0.106381 \n",
            "loss at epoch 5520: 0.662540 \n",
            "loss at epoch 5540: 0.588253 \n",
            "loss at epoch 5560: 0.434572 \n",
            "loss at epoch 5580: 4.910936 \n",
            "loss at epoch 5600: 1.128745 \n",
            "loss at epoch 5620: 0.252169 \n",
            "loss at epoch 5640: 0.142476 \n",
            "loss at epoch 5660: 0.802739 \n",
            "loss at epoch 5680: 0.255124 \n",
            "loss at epoch 5700: 0.114615 \n",
            "loss at epoch 5720: 0.319661 \n",
            "loss at epoch 5740: 0.306229 \n",
            "loss at epoch 5760: 0.107271 \n",
            "loss at epoch 5780: 2.386938 \n",
            "loss at epoch 5800: 0.690353 \n",
            "loss at epoch 5820: 0.193919 \n",
            "loss at epoch 5840: 0.078420 \n",
            "loss at epoch 5860: 1.885081 \n",
            "loss at epoch 5880: 1.434320 \n",
            "loss at epoch 5900: 0.346717 \n",
            "loss at epoch 5920: 1.238155 \n",
            "loss at epoch 5940: 0.270993 \n",
            "loss at epoch 5960: 0.246473 \n",
            "loss at epoch 5980: 0.122793 \n",
            "loss at epoch 6000: 0.291013 \n",
            "loss at epoch 6020: 0.269394 \n",
            "loss at epoch 6040: 3.179930 \n",
            "loss at epoch 6060: 0.785572 \n",
            "loss at epoch 6080: 0.178816 \n",
            "loss at epoch 6100: 1.061790 \n",
            "loss at epoch 6120: 1.175803 \n",
            "loss at epoch 6140: 0.194039 \n",
            "loss at epoch 6160: 0.090153 \n",
            "loss at epoch 6180: 0.050763 \n",
            "loss at epoch 6200: 0.044173 \n",
            "loss at epoch 6220: 0.054888 \n",
            "loss at epoch 6240: 0.361677 \n",
            "loss at epoch 6260: 0.185096 \n",
            "loss at epoch 6280: 0.040200 \n",
            "loss at epoch 6300: 0.207550 \n",
            "loss at epoch 6320: 1.178272 \n",
            "loss at epoch 6340: 0.349945 \n",
            "loss at epoch 6360: 0.176984 \n",
            "loss at epoch 6380: 0.148478 \n",
            "loss at epoch 6400: 0.853145 \n",
            "loss at epoch 6420: 0.292886 \n",
            "loss at epoch 6440: 0.106411 \n",
            "loss at epoch 6460: 0.071397 \n",
            "loss at epoch 6480: 3.277748 \n",
            "loss at epoch 6500: 1.071878 \n",
            "loss at epoch 6520: 0.177220 \n",
            "loss at epoch 6540: 0.056627 \n",
            "loss at epoch 6560: 0.045824 \n",
            "loss at epoch 6580: 1.454904 \n",
            "loss at epoch 6600: 0.046250 \n",
            "loss at epoch 6620: 0.052212 \n",
            "loss at epoch 6640: 0.260489 \n",
            "loss at epoch 6660: 0.070315 \n",
            "loss at epoch 6680: 0.313459 \n",
            "loss at epoch 6700: 2297.129883 \n",
            "loss at epoch 6720: 208.234894 \n",
            "loss at epoch 6740: 54.230469 \n",
            "loss at epoch 6760: 22.267735 \n",
            "loss at epoch 6780: 12.222610 \n",
            "loss at epoch 6800: 8.077843 \n",
            "loss at epoch 6820: 5.896564 \n",
            "loss at epoch 6840: 4.600412 \n",
            "loss at epoch 6860: 3.751187 \n",
            "loss at epoch 6880: 3.146128 \n",
            "loss at epoch 6900: 2.701794 \n",
            "loss at epoch 6920: 2.378380 \n",
            "loss at epoch 6940: 2.074622 \n",
            "loss at epoch 6960: 1.968470 \n",
            "loss at epoch 6980: 1.660864 \n",
            "loss at epoch 7000: 1.502256 \n",
            "loss at epoch 7020: 1.378112 \n",
            "loss at epoch 7040: 1.478749 \n",
            "loss at epoch 7060: 1.171155 \n",
            "loss at epoch 7080: 1.099669 \n",
            "loss at epoch 7100: 0.967178 \n",
            "loss at epoch 7120: 1.131037 \n",
            "loss at epoch 7140: 0.831919 \n",
            "loss at epoch 7160: 0.777045 \n",
            "loss at epoch 7180: 0.729824 \n",
            "loss at epoch 7200: 0.707137 \n",
            "loss at epoch 7220: 0.656502 \n",
            "loss at epoch 7240: 0.635260 \n",
            "loss at epoch 7260: 0.584231 \n",
            "loss at epoch 7280: 0.743635 \n",
            "loss at epoch 7300: 0.654434 \n",
            "loss at epoch 7320: 0.512406 \n",
            "loss at epoch 7340: 0.461419 \n",
            "loss at epoch 7360: 0.583798 \n",
            "loss at epoch 7380: 0.697746 \n",
            "loss at epoch 7400: 0.445048 \n",
            "loss at epoch 7420: 0.387825 \n",
            "loss at epoch 7440: 0.490650 \n",
            "loss at epoch 7460: 2.050415 \n",
            "loss at epoch 7480: 0.667097 \n",
            "loss at epoch 7500: 0.335228 \n",
            "loss at epoch 7520: 0.314942 \n",
            "loss at epoch 7540: 0.301356 \n",
            "loss at epoch 7560: 0.289491 \n",
            "loss at epoch 7580: 0.754078 \n",
            "loss at epoch 7600: 0.505506 \n",
            "loss at epoch 7620: 0.276889 \n",
            "loss at epoch 7640: 0.261583 \n",
            "loss at epoch 7660: 0.243905 \n",
            "loss at epoch 7680: 0.427862 \n",
            "loss at epoch 7700: 0.562597 \n",
            "loss at epoch 7720: 0.356659 \n",
            "loss at epoch 7740: 0.220064 \n",
            "loss at epoch 7760: 0.248722 \n",
            "loss at epoch 7780: 1.196298 \n",
            "loss at epoch 7800: 0.288132 \n",
            "loss at epoch 7820: 0.220817 \n",
            "loss at epoch 7840: 0.205027 \n",
            "loss at epoch 7860: 0.789365 \n",
            "loss at epoch 7880: 0.851842 \n",
            "loss at epoch 7900: 0.343684 \n",
            "loss at epoch 7920: 0.176402 \n",
            "loss at epoch 7940: 0.164930 \n",
            "loss at epoch 7960: 0.159818 \n",
            "loss at epoch 7980: 0.163395 \n",
            "loss at epoch 8000: 1.543161 \n",
            "loss at epoch 8020: 0.253347 \n",
            "loss at epoch 8040: 0.175924 \n",
            "loss at epoch 8060: 0.141308 \n",
            "loss at epoch 8080: 0.137904 \n",
            "loss at epoch 8100: 0.151334 \n",
            "loss at epoch 8120: 1.483281 \n",
            "loss at epoch 8140: 0.308069 \n",
            "loss at epoch 8160: 0.144618 \n",
            "loss at epoch 8180: 0.129160 \n",
            "loss at epoch 8200: 1.108748 \n",
            "loss at epoch 8220: 0.486326 \n",
            "loss at epoch 8240: 0.163908 \n",
            "loss at epoch 8260: 0.117962 \n",
            "loss at epoch 8280: 0.356444 \n",
            "loss at epoch 8300: 0.254920 \n",
            "loss at epoch 8320: 0.128245 \n",
            "loss at epoch 8340: 2.102531 \n",
            "loss at epoch 8360: 0.244357 \n",
            "loss at epoch 8380: 0.103759 \n",
            "loss at epoch 8400: 0.098135 \n",
            "loss at epoch 8420: 0.123479 \n",
            "loss at epoch 8440: 1.264727 \n",
            "loss at epoch 8460: 0.249708 \n",
            "loss at epoch 8480: 0.095852 \n",
            "loss at epoch 8500: 0.092653 \n",
            "loss at epoch 8520: 0.090471 \n",
            "loss at epoch 8540: 3.939366 \n",
            "loss at epoch 8560: 0.535525 \n",
            "loss at epoch 8580: 0.099454 \n",
            "loss at epoch 8600: 0.086855 \n",
            "loss at epoch 8620: 0.133341 \n",
            "loss at epoch 8640: 0.386902 \n",
            "loss at epoch 8660: 0.136863 \n",
            "loss at epoch 8680: 0.085835 \n",
            "loss at epoch 8700: 0.161477 \n",
            "loss at epoch 8720: 0.696515 \n",
            "loss at epoch 8740: 0.144457 \n",
            "loss at epoch 8760: 0.171606 \n",
            "loss at epoch 8780: 0.086524 \n",
            "loss at epoch 8800: 0.071150 \n",
            "loss at epoch 8820: 0.072595 \n",
            "loss at epoch 8840: 2.779949 \n",
            "loss at epoch 8860: 0.330703 \n",
            "loss at epoch 8880: 0.095437 \n",
            "loss at epoch 8900: 0.068873 \n",
            "loss at epoch 8920: 0.076137 \n",
            "loss at epoch 8940: 0.536417 \n",
            "loss at epoch 8960: 0.102418 \n",
            "loss at epoch 8980: 0.072981 \n",
            "loss at epoch 9000: 0.578396 \n",
            "loss at epoch 9020: 0.286848 \n",
            "loss at epoch 9040: 0.186172 \n",
            "loss at epoch 9060: 0.205477 \n",
            "loss at epoch 9080: 0.635248 \n",
            "loss at epoch 9100: 0.306474 \n",
            "loss at epoch 9120: 0.159209 \n",
            "loss at epoch 9140: 0.075164 \n",
            "loss at epoch 9160: 0.054266 \n",
            "loss at epoch 9180: 0.057689 \n",
            "loss at epoch 9200: 3.046259 \n",
            "loss at epoch 9220: 0.265991 \n",
            "loss at epoch 9240: 0.058748 \n",
            "loss at epoch 9260: 0.076063 \n",
            "loss at epoch 9280: 0.136220 \n",
            "loss at epoch 9300: 0.103907 \n",
            "loss at epoch 9320: 0.062618 \n",
            "loss at epoch 9340: 1.204482 \n",
            "loss at epoch 9360: 0.373393 \n",
            "loss at epoch 9380: 0.054854 \n",
            "loss at epoch 9400: 0.056492 \n",
            "loss at epoch 9420: 0.048170 \n",
            "loss at epoch 9440: 0.947586 \n",
            "loss at epoch 9460: 1.094152 \n",
            "loss at epoch 9480: 0.169940 \n",
            "loss at epoch 9500: 0.068355 \n",
            "loss at epoch 9520: 0.067954 \n",
            "loss at epoch 9540: 1.152383 \n",
            "loss at epoch 9560: 0.460366 \n",
            "loss at epoch 9580: 0.089967 \n",
            "loss at epoch 9600: 0.090083 \n",
            "loss at epoch 9620: 0.496866 \n",
            "loss at epoch 9640: 0.137695 \n",
            "loss at epoch 9660: 0.059719 \n",
            "loss at epoch 9680: 0.039588 \n",
            "loss at epoch 9700: 0.220460 \n",
            "loss at epoch 9720: 0.648304 \n",
            "loss at epoch 9740: 0.127514 \n",
            "loss at epoch 9760: 0.612687 \n",
            "loss at epoch 9780: 0.053923 \n",
            "loss at epoch 9800: 0.050784 \n",
            "loss at epoch 9820: 1.804481 \n",
            "loss at epoch 9840: 0.461902 \n",
            "loss at epoch 9860: 0.036396 \n",
            "loss at epoch 9880: 0.074883 \n",
            "loss at epoch 9900: 0.100694 \n",
            "loss at epoch 9920: 0.049448 \n",
            "loss at epoch 9940: 1.256215 \n",
            "loss at epoch 9960: 0.669444 \n",
            "loss at epoch 9980: 0.159336 \n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass the pseudonovel images to the MLP\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "X_train_pseudonovel = torch.tensor(df_Features_Unknown.values)\n",
        "\n",
        "X_train_novel= X_train_pseudonovel\n",
        "print(X_train_pseudonovel.shape)\n",
        "Genus_List_With_Pseudonovel = GenusList + [\"$\\it{Panama}$ (4.25)\"]\n",
        "\n",
        "len(Genus_List_With_Pseudonovel)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train_novel = X_train_novel.to(device)\n",
        "output = best_model(X_train.float())\n",
        "output_novel = best_model(X_train_novel.reshape(X_train_novel.shape[0],1,X_train_novel.shape[1]).float())\n",
        "output_novel.shape\n",
        "Dataset_With_Known_And_Novel = torch.vstack((output, output_novel))\n",
        "Dataset_With_Known_And_Novel.shape\n",
        "\n",
        "# k + n scheme\n",
        "# Average all features per known species\n",
        "\n",
        "Species_Index = Y;\n",
        "NumericSpeciesIndex = pd.to_numeric(Species_Index);\n",
        "Indices_and_Features = np.column_stack((NumericSpeciesIndex, output.cpu().detach().numpy()));\n",
        "df = pd.DataFrame(data=Indices_and_Features)\n",
        "Per_Species_Transformed_Features = df.groupby([0,]).mean()\n",
        "Per_Species_Transformed_Features;\n",
        "\n",
        "NumericSpeciesIndexNovel  = [\"$\\it{Panama}$ (4.25)\"]\n",
        "\n",
        "Indices_and_Features_Novel = np.column_stack((NumericSpeciesIndexNovel, output_novel.cpu().detach().numpy()));\n",
        "df_novel = pd.DataFrame(data= Indices_and_Features_Novel)\n",
        "df_novel.groupby(by=0,axis = 0)\n",
        "\n",
        "df_known_averaged_and_Novel = pd.concat([Per_Species_Transformed_Features,df_novel])\n",
        "df_known_averaged_and_Novel  = df_known_averaged_and_Novel.iloc[: , 1:]\n",
        "\n",
        "#  Obtain predicted cladogram (phylogenetic tree) with the placement of the pseudo-novel specimens \n",
        "import matplotlib\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "df_known_averaged_and_Novel_norm = (df_known_averaged_and_Novel-df_known_averaged_and_Novel.min())/(df_known_averaged_and_Novel.max()-df_known_averaged_and_Novel.min())\n",
        "linked = linkage(df_known_averaged_and_Novel, \"average\", metric = \"cosine\")\n",
        "\n",
        "Genera = [\"$\\it{P. acutifolius}$\",\"$\\it{P. archboldii}$\",\"$\\it{P. brasiliensis}$\",\"$\\it{P. brassii}$\",\"$\\it{P. coriaceus}$\",\"$\\it{P. drouynianus}$\",\"$\\it{P. elatus}$\",\"$\\it{P. elongatus}$\",\"$\\it{P. glaucus}$\",\"$\\it{P. gnidiodes}$\",\"$\\it{P. lambertii}$\",\"$\\it{P. lawrencei}$\",\"$\\it{P. lucienii}$\",\"$\\it{P. macrophyllus}$\",\"$\\it{P. matudae}$\",\"$\\it{P. milanjianus}$\",\"$\\it{P. neriifolius}$\",\"$\\it{P. nivalis}$\",\"$\\it{P. novae-caledoniae}$\",\"$\\it{P. nubigenus}$\",\"$\\it{P. oleifolius}$\",\"$\\it{P. parlatorei}$\",\"$\\it{P. pilgeri}$\",\"$\\it{P. polystachyus}$\",\"$\\it{P. salignus}$\",\"$\\it{P. sellowii}$\",\"$\\it{P. spinulosus}$\",\"$\\it{P. sylvestris}$\",\"$\\it{P. totara}$\",\"$\\it{P. urbanii}$\",\"$\\it{Panama}$ (4.25)\"];\n",
        "\n",
        "labelList = Genera\n",
        "plt.figure(figsize=(5, 8), dpi = 100)\n",
        "matplotlib.rc('xtick', labelsize=8) \n",
        "matplotlib.rc('ytick', labelsize=8) \n",
        "dendrogram(linked,\n",
        "            orientation='left',\n",
        "            labels=labelList,\n",
        "            distance_sort='descending',\n",
        "            show_leaf_counts=True)\n",
        "plt.savefig(\"Fossil_placement_Panama_4.25.pdf\", bbox_inches = \"tight\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "ZRPYmBw8ci8m",
        "outputId": "a84d9313-ed2a-44bd-c8f5-fde8abbcc9be"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6144, 1])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and save cosine dissimilarity matrix as csv file. This matrix depicts distances separating the fossil specimen from the known taxa.\n",
        "Feature_Matrix_Known_and_Unknown = np.array(df_known_averaged_and_Novel)\n",
        "Feature_Matrix_Known_and_Unknown = Feature_Matrix_Known_and_Unknown.astype(float)\n",
        "Feature_Matrix_Known_and_Unknown = torch.tensor(Feature_Matrix_Known_and_Unknown)\n",
        "Feature_Matrix_Known_and_Unknown_dist = 1-sim_matrix(Feature_Matrix_Known_and_Unknown,Feature_Matrix_Known_and_Unknown)\n",
        "np.savetxt(absolute_path + \"/dist_matrix_fossil_placement.csv\", Feature_Matrix_Known_and_Unknown_dist, delimiter=\",\")"
      ],
      "metadata": {
        "id": "tku8ip95qnag"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
